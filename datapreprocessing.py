i# -*- coding: utf-8 -*-
"""DataPreprocessing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tTM-kPiJFWS7B2lLf4OeaekGNa9LWIWJ
"""
# ====================================================================================
#   runthis file 3 times to that ~2400 negatomes with structural descriptors is gotten
#  HD_part8_20230317_matrix_orthosteric__complete.csv  ===> pretraining ===> pretraining
#  HD_part8_20230317_matrix_orthosteric ===> pretraining2 ===>
#  HD-PL_part8_20230317_matrix_liganded_allosteric ===>pretraining3 ===> pretraining

#=======================================================================================
from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import re


interactome_csv_path = '/content/drive/MyDrive/datasets/HD_part8_20230317_matrix_orthosteric__complete.csv'
# Read with pandas
interactome_df = pd.read_csv(interactome_csv_path)
print("interactome dataset loaded:", interactome_df.shape)
print(interactome_df.columns[:10])

!pip install biopython
from Bio import SeqIO
from urllib.request import urlopen
import io

def get_sequence_biopython(uniprot_id):
    url = f"https://rest.uniprot.org/uniprotkb/{uniprot_id}.fasta"
    with urlopen(url) as handle:
        text_handle = io.TextIOWrapper(handle, encoding='utf-8')
        record = next(SeqIO.parse(text_handle, "fasta"))
        return str(record.seq)

# Example:
print(get_sequence_biopython("Q9DBR4"))

def extract_uniprot_ids(cavity_string):
    """
    Extracts UniProt IDs from a string using a regular expression.

    Args:
        cavity_string: The input string from the 'Cavity' column.

    Returns:
        A list of extracted UniProt IDs.
    """
    if isinstance(cavity_string, str):
      return re.findall(r'[OPQ][0-9][A-Z0-9]{3}[0-9]', str(cavity_string))
    return []

interactome_df['UniProt_IDs'] = interactome_df['Cavity'].apply(extract_uniprot_ids)

interactome_df['UniProt_ID'] = None
interactome_df['SequenceA'] = None
interactome_df['SequenceB'] = None
display(interactome_df[['Cavity', 'UniProt_IDs', 'UniProt_ID', 'SequenceA', 'SequenceB']].head())

interactome_df_subset = interactome_df.copy()

sequence_cache = {}

def fetch_sequence(uniprot_id):
    if uniprot_id not in sequence_cache:
        try:
            sequence_cache[uniprot_id] = get_sequence_biopython(uniprot_id)
        except Exception as e:
            print(f"Error fetching sequence for {uniprot_id}: {e}")
            sequence_cache[uniprot_id] = None
    return sequence_cache[uniprot_id]

def assign_sequences(row):
    uni_ids = row['UniProt_IDs']
    if not uni_ids or not isinstance(uni_ids, list):
        return pd.Series({'UniProt_ID': None, 'SequenceA': None, 'SequenceB': None})

    out = {'UniProt_ID': uni_ids[0], 'SequenceA': fetch_sequence(uni_ids[0]), 'SequenceB': None}
    if len(uni_ids) > 1:
        out['SequenceB'] = fetch_sequence(uni_ids[1])
    return pd.Series(out)

interactome_df_subset[['UniProt_ID', 'SequenceA', 'SequenceB']] = interactome_df_subset.apply(assign_sequences, axis=1)

all_sequences = pd.concat([interactome_df_subset['SequenceA'], interactome_df_subset['SequenceB']]).dropna()

all_sequences = all_sequences[all_sequences.apply(lambda x: isinstance(x, str))]

colabfold_sequences = list(set(all_sequences))


print(f"Number of unique sequences for ColabFold: {len(colabfold_sequences)}")

# Commented out IPython magic to ensure Python compatibility.
# #@title Install dependencies
# %%time
# import os
# 
# # Define variables with default values if not already defined
# USE_AMBER = use_amber if 'use_amber' in locals() else False
# USE_TEMPLATES = use_templates if 'use_templates' in locals() else False
# PYTHON_VERSION = python_version if 'python_version' in locals() else '3.11' # or your preferred default Python version
# 
# 
# if not os.path.isfile("COLABFOLD_READY"):
#   print("installing colabfold...")
#   os.system("pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'")
#   if os.environ.get('TPU_NAME', False) != False:
#     os.system("pip uninstall -y jax jaxlib")
#     os.system("pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html")
#   os.system("ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold")
#   os.system("ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold")
#   # hack to fix TF crash
#   os.system("rm -f /usr/local/lib/python3.*/dist-packages/tensorflow/core/kernels/libtfkernel_sobol_op.so")
#   os.system("touch COLABFOLD_READY")
# 
# if USE_AMBER or USE_TEMPLATES:
#   if not os.path.isfile("CONDA_READY"):
#     print("installing conda...")
#     os.system("wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh")
#     os.system("bash Miniforge3-Linux-x86_64.sh -bfp /usr/local")
#     os.system("mamba config --set auto_update_conda false")
#     os.system("touch CONDA_READY")
# 
# if USE_TEMPLATES and not os.path.isfile("HH_READY") and USE_AMBER and not os.path.isfile("AMBER_READY"):
#   print("installing hhsuite and amber...")
#   os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer")
#   os.system("touch HH_READY")
#   os.system("touch AMBER_READY")
# else:
#   if USE_TEMPLATES and not os.path.isfile("HH_READY"):
#     print("installing hhsuite...")
#     os.system(f"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'")
#     os.system("touch HH_READY")
#   if USE_AMBER and not os.path.isfile("AMBER_READY"):
#     print("installing amber...")
#     os.system(f"mamba install -y -c conda-forge openmm=8.2.0 python='{PYTHON_VERSION}' pdbfixer")
#     os.system("touch AMBER_READY")

import os
from colabfold.batch import get_msa_and_templates
from colabfold.utils import setup_logging
from pathlib import Path # Import Path

output_dir = 'colabfold_run_results'
os.makedirs(output_dir, exist_ok=True)
log_file_path_str = os.path.join(output_dir, 'colabfold_run.log')
# Convert the log file path string to a Path object
log_file_path = Path(log_file_path_str)
setup_logging(log_file_path)

result_dir_output = None


if 'colabfold_sequences' not in globals() or not colabfold_sequences:
    print("Error: colabfold_sequences list is not populated. Please run the cell to prepare unique sequences first.")
else:
    queries = []
    for seq in colabfold_sequences:
        if isinstance(seq, str) and seq:
            queries.append(("sequence", seq))

    if not queries:
         print("Error: No valid sequences found in colabfold_sequences to create queries.")
    else:
        result_dir_output = get_msa_and_templates(
            jobname="colabfold_run",
            query_sequences=queries,
            a3m_lines = None,
            result_dir=output_dir,
            use_templates=False,
            msa_mode="single_sequence",
            custom_template_path = None,
            pair_mode="unpaired_paired",
            pairing_strategy = "greedy",
            host_url = "https://api.colabfold.com",
            user_agent = "",
        )

        print(f"ColabFold run finished. Results are in: {result_dir_output}")

print(colabfold_sequences)

if 'result_dir_output' in globals() and isinstance(result_dir_output, tuple) and len(result_dir_output) > 0:
    raw_msa_output_list = result_dir_output[0]
    msa_dict_from_run = {}
    if 'colabfold_sequences' in globals() and len(colabfold_sequences) == len(raw_msa_output_list):
        for i, seq in enumerate(colabfold_sequences):
            msa_dict_from_run[seq] = raw_msa_output_list[i]
        print(f"Created MSA dictionary from ColabFold run output for {len(msa_dict_from_run)} unique sequences.")
    else:
        print("Error: Mismatch between number of unique sequences and run output MSAs.")
        msa_dict_from_run = {}
    if 'MSAA' not in interactome_df_subset.columns:
        interactome_df_subset.loc[:, 'MSAA'] = None
    if 'MSAB' not in interactome_df_subset.columns:
        interactome_df_subset.loc[:, 'MSAB'] = None


    for index, row in interactome_df_subset.iterrows():
        seq_a = row['SequenceA']
        seq_b = row['SequenceB']

        if seq_a and seq_a in msa_dict_from_run:
            interactome_df_subset.loc[index, 'MSAA'] = msa_dict_from_run[seq_a]
        else:
            if interactome_df_subset.loc[index, 'MSAA'] is not None:
                 interactome_df_subset.loc[index, 'MSAA'] = None


        if seq_b and seq_b in msa_dict_from_run:
            interactome_df_subset.loc[index, 'MSAB'] = msa_dict_from_run[seq_b]
        else:
            if interactome_df_subset.loc[index, 'MSAB'] is not None:
                 interactome_df_subset.loc[index, 'MSAB'] = None

    print("MSAs mapped to DataFrame subset.")

    display(interactome_df_subset[['Cavity', 'UniProt_IDs', 'UniProt_ID', 'SequenceA', 'SequenceB', 'MSAA', 'MSAB']].tail(2))

else:
    print("ColabFold run output (result_dir_output) is not available or not in expected format.")

missing_msa_a = interactome_df_subset[interactome_df_subset['MSAA'].isna()]
missing_msa_b = interactome_df_subset[interactome_df_subset['MSAB'].isna()]

print(f"Number of rows with missing MSAA: {len(missing_msa_a)}")
print(f"Number of rows with missing MSAB: {len(missing_msa_b)}")

if not missing_msa_a.empty:
    print("\nRows with missing MSAA:")
    display(missing_msa_a[['Cavity', 'UniProt_IDs', 'UniProt_ID', 'SequenceA', 'MSAA']].head())

if not missing_msa_b.empty:
    print("\nRows with missing MSAB:")
    display(missing_msa_b[['Cavity', 'UniProt_IDs', 'UniProt_ID', 'SequenceB', 'MSAB']].head())

print(interactome_df.columns.tolist())

available_columns = interactome_df.columns.tolist()

structural_features_columns_refined = [col for col in ['Volume', 'CZ', 'CA', 'O', 'OD1', 'OG', 'N', 'NZ', 'DU', 'CZ40', 'CZ40-50',
                                                       'CZ50-60', 'CZ60-70', 'CZ70-80', 'CZ80-90', 'CZ90-100', 'CZ100-110', 'CZ110-120',
                                                       'CZ120', 'CA40', 'CA40-50', 'CA50-60', 'CA60-70', 'CA70-80', 'CA80-90',
                                                       'CA90-100', 'CA100-110', 'CA110-120', 'CA120', 'O40', 'O40-50', 'O50-60',
                                                       'O60-70', 'O70-80', 'O80-90', 'O90-100', 'O100-110', 'O110-120', 'O120',
                                                       'OD140', 'OD140-50', 'OD150-60', 'OD160-70', 'OD170-80', 'OD180-90',
                                                       'OD190-100', 'OD1100-110', 'OD1110-120', 'OD1120', 'OG40', 'OG40-50',
                                                       'OG50-60', 'OG60-70', 'OG70-80', 'OG80-90', 'OG90-100', 'OG100-110',
                                                       'OG110-120', 'OG120', 'N40', 'N40-50', 'N50-60', 'N60-70', 'N70-80',
                                                       'N80-90', 'N90-100', 'N100-110', 'N110-120', 'N120', 'NZ40', 'NZ40-50',
                                                       'NZ50-60', 'NZ60-70', 'NZ70-80', 'NZ80-90', 'NZ90-100', 'NZ100-110',
                                                       'NZ110-120', 'NZ120', 'DU40', 'DU40-50', 'DU50-60', 'DU60-70', 'DU70-80',
                                                       'DU80-90', 'DU90-100', 'DU100-110', 'DU110-120', 'DU120',
                                                       'T40', 'T40-50', 'T50-60', 'T60-70', 'T70-80', 'T80-90', 'T90-100',
                                                       'T100-110', 'T110-120', 'PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2', 'Rgyr',
                                                       'Asphericity', 'SpherocityIndex', 'Eccentricity', 'InertialShapeFactor'] if col in available_columns]


structural_features_df_refined = interactome_df[structural_features_columns_refined]


structural_features_subset = structural_features_df_refined.loc[interactome_df_subset.index]

display(structural_features_subset.head())

potential_numeric_structural_columns = [col for col in interactome_df_subset.columns if col not in [
    'cath', 'pdb.chain', 'Cavity', 'pdb_code_index', 'chain_index', 'pfam_accession',
    'pfam_name', 'class', 'architecture', 'topology', 'homologous', 'cath_name',
    'UniProt_IDs', 'UniProt_ID', 'SequenceA', 'SequenceB', 'MSAA', 'MSAB'
]]

for col in potential_numeric_structural_columns:
    interactome_df_subset[col] = pd.to_numeric(interactome_df_subset[col], errors='coerce')


for col in potential_numeric_structural_columns:
    interactome_df_subset.loc[:, col] = interactome_df_subset.loc[:, col].fillna(0)


sequence_msa_cols = ['SequenceA', 'SequenceB', 'MSAA', 'MSAB']
for col in sequence_msa_cols:
     if col in interactome_df_subset.columns:
        interactome_df_subset.loc[:, col] = interactome_df_subset.loc[:, col].fillna('')



print("Remaining missing values per column after handling:")
display(interactome_df_subset.isnull().sum())

pretraining_columns = ['UniProt_ID', 'SequenceA', 'SequenceB', 'MSAA', 'MSAB'] + structural_features_columns_refined
interactome_df_pretraining = interactome_df_subset.loc[:, pretraining_columns]
display(interactome_df_pretraining.head())

interactome_df_pretraining['label'] = 1
display(interactome_df_pretraining.head())

display(interactome_df_subset.shape)

import pandas as pd

splits = {
    'combined': 'data/combined-00000-of-00001.parquet',
    'combined_stringent': 'data/combined_stringent-00000-of-00001.parquet',
    'manual': 'data/manual-00000-of-00001.parquet',
    'manual_stringent': 'data/manual_stringent-00000-of-00001.parquet',
    'pdb': 'data/pdb-00000-of-00001.parquet',
    'pdb_stringent': 'data/pdb_stringent-00000-of-00001.parquet'
}

dataframes = []
for split_name, file_path in splits.items():
    df = pd.read_parquet(f"hf://datasets/Synthyra/NEGATOME/{file_path}")
    dataframes.append(df)
    print(f"Loaded {split_name}: {df.shape}")

merged_df = pd.concat(dataframes, ignore_index=True)
print(f"\nMerged dataset shape: {merged_df.shape}")
merged_df.to_csv('/content/drive/MyDrive/negatome1.csv', index=False)

negatome_csv_path = '/content/drive/MyDrive/negatome1.csv'
negatome_df = pd.read_csv(negatome_csv_path, low_memory=False)
display(negatome_df.head())
# Change A, SeqA and SeqB to Uniprot_ID SequenceA  and SequenceB
negatome_df.rename(columns={'A': 'UniProt_ID','SeqA': 'SequenceA', 'SeqB': 'SequenceB'}, inplace=True)
display(negatome_df.head())

# Prepare a lookup dictionary for UniProt_ID to structural features
columns_to_copy = structural_features_columns_refined + ['UniProt_ID']
interactome_subset = interactome_df_pretraining[columns_to_copy].drop_duplicates(subset=['UniProt_ID'])
feature_dict = interactome_subset.set_index('UniProt_ID')[structural_features_columns_refined].to_dict(orient='index')

# Create empty columns in negatome_df for the features
for col in structural_features_columns_refined:
    negatome_df[col] = None

# Use iterrows to populate structural features
for idx, row in negatome_df.iterrows():
    uni_id = row['UniProt_ID']
    if uni_id in feature_dict:
        for feat_col in structural_features_columns_refined:
            negatome_df.at[idx, feat_col] = feature_dict[uni_id][feat_col]

# Display the head of the updated DataFrame
print("Negatome DataFrame after manual iterrows merge:")
display(negatome_df.head())

# Define the columns to copy from interactome_df_pretraining (structural features)
columns_to_copy = structural_features_columns_refined + ['UniProt_ID'] # Include UniProt_ID for the merge

# Select only the necessary columns from interactome_df_pretraining for the merge
interactome_subset_for_merge = interactome_df_pretraining[columns_to_copy].drop_duplicates(subset=['UniProt_ID']).copy()

# Drop structural feature columns from the original negatome_df before merging
negatome_df_structural_dropped = negatome_df.drop(columns=structural_features_columns_refined, errors='ignore').copy()

# Merge negatome_df with the selected structural features based on UniProt_ID
# Use a left merge to keep all rows from negatome_df
negatome_df_merged = pd.merge(negatome_df_structural_dropped, interactome_subset_for_merge, on='UniProt_ID', how='left')

# Display the head of the merged negatome_df
print("Negatome DataFrame with copied structural features (merged):")
display(negatome_df_merged.head())

missing_volume = negatome_df_merged[negatome_df_merged['Volume'].isna()]

print(f"Number of rows with missing Volume: {len(missing_volume)}")

# Display rows with missing Volume (if any)
if not missing_volume.empty:
    print("\nRows with missing Volume:")
    display(missing_volume[['UniProt_ID', 'SequenceA', 'Volume']].head())
# Add a new column with label "0" to show non-interaction
negatome_df_merged['label'] = 0

# Drop N/A values in negatome_df_merged
# Use .loc to avoid SettingWithCopyWarning
for col in structural_features_columns_refined:
    if col in negatome_df_merged.columns:
        negatome_df_merged.loc[:, col] = negatome_df_merged.loc[:, col].dropna()


# Display the head of the negatome_df_merged DataFrame with filled NaNs and the new column
print("\nNegatome DataFrame after filling NaNs and adding label:")
display(negatome_df_merged.head())
# Ensure msa_dict_from_run is available from previous ColabFold run
if 'msa_dict_from_run' not in globals():
    print("Error: msa_dict_from_run not found. Please run the ColabFold cell first.")
else:
    negatome_df_merged.loc[:, 'MSAA'] = None
    negatome_df_merged.loc[:, 'MSAB'] = None

    # Define a function to fetch MSA from the dictionary
    def fetch_msa_from_dict(sequence):
        if sequence and sequence in msa_dict_from_run:
            return msa_dict_from_run[sequence]
        return None

    # Iterate through negatome_df_merged and populate MSAA and MSAB
    for index, row in negatome_df_merged.iterrows():
        seq_a = row['SequenceA']
        seq_b = row['SequenceB']

        # Fetch and assign MSA for SequenceA
        msa_a = fetch_msa_from_dict(seq_a)
        if msa_a:
            negatome_df_merged.loc[index, 'MSAA'] = msa_a

        # Fetch and assign MSA for SequenceB
        msa_b = fetch_msa_from_dict(seq_b)
        if msa_b:
            negatome_df_merged.loc[index, 'MSAB'] = msa_b

    cols = negatome_df_merged.columns.tolist()
    if 'MSAA' in cols:
        cols.remove('MSAA')
    if 'MSAB' in cols:
        cols.remove('MSAB')
    # Find the index of SequenceB
    seq_b_index = cols.index('SequenceB')
    # Insert MSAA and MSAB after SequenceB
    cols.insert(seq_b_index + 1, 'MSAA')
    cols.insert(seq_b_index + 2, 'MSAB')

    # Reindex the DataFrame to the new column order
    negatome_df_merged = negatome_df_merged[cols]

    # Drop the original 'B' column
    if 'B' in negatome_df_merged.columns:
        negatome_df_merged = negatome_df_merged.drop(columns=['B'])

    print("\nNegatome DataFrame after fetching MSAs, reordering columns, and dropping 'B' column:")
    display(negatome_df_merged.head())

# Check if columns match before concatenation
if set(interactome_df_pretraining.columns) == set(negatome_df_merged.columns):
    pretraining_dataset = pd.concat([interactome_df_pretraining, negatome_df_merged], ignore_index=True)

    # Display the head and the shape of the combined dataset
    print("\nCombined pretraining dataset:")
    display(pretraining_dataset.head())
    print("\nShape of the combined pretraining dataset:", pretraining_dataset.shape)

else:
    print("Column mismatch between interactome and negatome DataFrames. Cannot concatenate.")
    print("Columns in interactome_df_pretraining:", interactome_df_pretraining.columns.tolist())
    print("Columns in negatome_df_merged:", negatome_df_merged.columns.tolist())

# Drop N/A values
pretraining_dataset.dropna(inplace=True)

pretraining_dataset.reset_index(drop=True, inplace=True)
display(pretraining_dataset.shape)

# save final dataset to drive
pretraining_dataset.to_csv('/content/drive/MyDrive/pretraining.csv', index=False)

#
alz_df = pd.read_csv('/content/drive/MyDrive/Alzheimer.tsv', sep='\t', dtype=str, engine='python')

print(alz_df.columns.tolist())

def extract_uniprot_from_mitab(s):
    if pd.isna(s):
        return None
    match = re.search(r'uniprotkb:([A-Za-z0-9\-]+)', str(s))
    return match.group(1) if match else None

alz_df['UniProt_A'] = alz_df['# ID(s) interactor A'].apply(extract_uniprot_from_mitab)
alz_df['UniProt_B'] = alz_df['ID(s) interactor B'].apply(extract_uniprot_from_mitab)

print(alz_df[['# ID(s) interactor A', 'ID(s) interactor B', 'UniProt_A', 'UniProt_B']].head(17))

# Get unique UniProt IDs from both columns in the Alzheimer's dataset
unique_alz_uniprot_ids = pd.concat([alz_df['UniProt_A'], alz_df['UniProt_B']]).dropna().unique().tolist()

print(f"Number of unique UniProt IDs in Alzheimer's dataset: {len(unique_alz_uniprot_ids)}")

alz_sequence_cache = {} # Create a new cache for Alzheimer's sequences

def fetch_alz_sequence(uniprot_id):
    if uniprot_id not in alz_sequence_cache:
        try:
            # Reuse the get_sequence_biopython function
            alz_sequence_cache[uniprot_id] = get_sequence_biopython(uniprot_id)
        except Exception as e:
            print(f"Error fetching sequence for {uniprot_id}: {e}")
            alz_sequence_cache[uniprot_id] = None
    return alz_sequence_cache[uniprot_id]

# Add SequenceA and SequenceB columns to alz_df based on fetched sequences
alz_df['SequenceA'] = alz_df['UniProt_A'].apply(fetch_alz_sequence)
alz_df['SequenceB'] = alz_df['UniProt_B'].apply(fetch_alz_sequence)

print("\nAlzheimer's DataFrame with fetched sequences:")
display(alz_df[['UniProt_A', 'UniProt_B', 'SequenceA', 'SequenceB']].head())

alz = alz_df[['UniProt_A', 'UniProt_B', 'SequenceA', 'SequenceB']]
# save final dataset to drive
alz.to_csv('/content/drive/MyDrive/Alzheimer.csv', index=False)

import pandas as pd
import os

def merge_pretraining_data():
    """
    Merges all rows with label 0 from pretraining2.csv and pretraining3.csv
    with the entire pretraining.csv dataset and saves the result as final_Pretraining.csv
    """
    print("Loading datasets...")

    try:
        main_df = pd.read_csv('/content/drive/MyDrive/pretraining.csv')
        print(f"Main dataset loaded: {main_df.shape[0]} rows, {main_df.shape[1]} columns")
    except Exception as e:
        print(f"Error loading pretraining.csv: {str(e)}")
        return

    try:
        df2 = pd.read_csv('/content/drive/MyDrive/pretraining2.csv')
        df2_filtered = df2[df2['label'] == 0].copy()
        print(f"Dataset 2: {df2.shape[0]} rows total, {df2_filtered.shape[0]} rows with label 0")
    except Exception as e:
        print(f"Error processing pretraining2.csv: {str(e)}")
        df2_filtered = pd.DataFrame()

    try:
        df3 = pd.read_csv('/content/drive/MyDrive/pretraining3.csv')
        df3_filtered = df3[df3['label'] == 0].copy()
        print(f"Dataset 3: {df3.shape[0]} rows total, {df3_filtered.shape[0]} rows with label 0")
    except Exception as e:
        print(f"Error processing pretraining3.csv: {str(e)}")
        df3_filtered = pd.DataFrame()

    main_columns = set(main_df.columns)

    if not df2_filtered.empty:
        df2_columns = set(df2_filtered.columns)
        missing_cols = main_columns - df2_columns
        if missing_cols:
            print(f"Warning: pretraining2.csv is missing these columns: {missing_cols}")
            # Add missing columns with NaN values
            for col in missing_cols:
                df2_filtered[col] = pd.NA
        # Ensure column order matches main_df
        df2_filtered = df2_filtered[main_df.columns]

    if not df3_filtered.empty:
        df3_columns = set(df3_filtered.columns)
        missing_cols = main_columns - df3_columns
        if missing_cols:
            print(f"Warning: pretraining3.csv is missing these columns: {missing_cols}")
            # Add missing columns with NaN values
            for col in missing_cols:
                df3_filtered[col] = pd.NA
        # Ensure column order matches main_df
        df3_filtered = df3_filtered[main_df.columns]

    # Merge datasets
    print("Merging datasets...")
    dfs_to_merge = [main_df]
    if not df2_filtered.empty:
        dfs_to_merge.append(df2_filtered)
    if not df3_filtered.empty:
        dfs_to_merge.append(df3_filtered)

    final_df = pd.concat(dfs_to_merge, ignore_index=True)

    # Save merged dataset
    try:
        final_df.to_csv('final_Pretraining.csv', index=False)
        print(f"Successfully saved final_Pretraining.csv with {final_df.shape[0]} rows")

        # Print class distribution
        if 'label' in final_df.columns:
            print("\nLabel distribution in final dataset:")
            print(final_df['label'].value_counts())

        # Calculate how many new rows were added
        added_rows = final_df.shape[0] - main_df.shape[0]
        print(f"\nAdded {added_rows} rows with label 0 from supplementary datasets")

    except Exception as e:
        print(f"Error saving final_Pretraining.csv: {str(e)}")

# Run the merge function
if __name__ == "__main__":
    merge_pretraining_data()