# -*- coding: utf-8 -*-
"""dissertation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ES5d8WMyYg3lUatzREHEEfH5Z_a7FeK
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import random
import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, precision_recall_curve # Import necessary sklearn metrics
import matplotlib.pyplot as plt
from tqdm import tqdm
import seaborn as sns
from torch.nn.utils import spectral_norm
from sklearn.metrics import average_precision_score, precision_recall_curve
from torch.nn.attention import sdpa_kernel, SDPBackend
import os
from sklearn.preprocessing import StandardScaler
from torch.utils.data import WeightedRandomSampler
from difflib import SequenceMatcher

# Set device
USE_CUDA = True
device = torch.device("cuda" if (USE_CUDA and torch.cuda.is_available()) else "cpu")
torch.backends.cudnn.benchmark = True
print(f"Using device: {device}")

# Mount Google Drive for Colab
# from google.colab import drive
# drive.mount('/content/drive')
try:
    from google.colab import drive as _colab_drive
    IN_COLAB = True
    _colab_drive.mount('/content/drive')
except Exception:
    IN_COLAB = False

# Global plotting theme (single source of truth)
COLOR_PALETTE = ["#cc0000", "#990000", "#000000", "#550000"]
FACE_COLOR = "#f6f5f5"

# + output helpers (Colab or local/Windows)
def get_output_dir():
    if IN_COLAB and os.path.isdir('/content/drive/MyDrive'):
        out = '/content/drive/MyDrive'
    else:
        out = os.getenv('OUTPUT_DIR', os.path.join(os.getcwd(), 'outputs'))
    os.makedirs(out, exist_ok=True)
    return out

def save_fig(filename, dpi=150):
    path = os.path.join(get_output_dir(), filename)
    plt.savefig(path, dpi=dpi)
    return path

# Key changes for stability:
SEED = 42
AMP_ENABLED = True
EMA_DECAY = 0.999
N_CRITIC = 4
G_LR = 1e-4
D_LR = 1e-4
G_BETAS = (0.5, 0.9)  # WGAN-GP standard
D_BETAS = (0.5, 0.9)  # WGAN-GP standard
D_MODEL = 256
NHEAD = 8
NUM_LAYERS = 4
DROPOUT = 0.1
LAMBDA_GP = 10.0  # Standard for WGAN-GP
GEN_LS = "-"
DISC_LS = "--"

def set_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

set_seed(SEED)

sns.set_theme(style="white")
sns.set_palette(COLOR_PALETTE)
plt.rcParams["axes.facecolor"] = FACE_COLOR
plt.rcParams["figure.facecolor"] = FACE_COLOR

# Distinct, explicit styles for GAN curves
GEN_COLOR = COLOR_PALETTE[0]
DISC_COLOR = COLOR_PALETTE[1]


def style_axes(ax):
    """Apply consistent axis styling."""
    ax.grid(False)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(True)
    ax.spines['bottom'].set_visible(False)
    ax.set_facecolor(FACE_COLOR)


class DirectSequenceEncoder(nn.Module):
    """
    Direct amino acid encoder using nn.Embedding over indexed tokens (PAD, UNK, 20 AAs),
    with packed LSTM and masked attention. Provides a fast one-hot helper for GAN.
    Args:
        latent_dim: Output latent feature dimension per sequence.
        hidden_dim: Hidden size per LSTM direction.
        num_layers: Number of LSTM layers.
        dropout: Dropout applied inside LSTM stack and MLP head.
    Notes:
        Vocabulary indices: PAD=0, UNK=1, AAs=2..21 (A..Y without B/J/O/U/X/Z).
        All tensors are moved to the current device.
    """
    def __init__(self, latent_dim=64, hidden_dim=256, num_layers=2, dropout=0.2):
        super().__init__()
        self.latent_dim = latent_dim
        self.hidden_dim = hidden_dim

        # Alphabet and vocabulary (PAD=0, UNK=1, AAs start at 2)
        self.amino_acids = "ACDEFGHIKLMNPQRSTVWY"  # 20
        self.pad_idx = 0
        self.unk_idx = 1
        self.aa_to_idx = {aa: i + 2 for i, aa in enumerate(self.amino_acids)}
        # Store the index for 'M' (Methionine)[most amino acid begins with M]
        self.m_idx = self.aa_to_idx.get('M', -1)
        if self.m_idx == -1:
            raise ValueError("Amino acid 'M' not found in the defined alphabet.")

        self.num_amino_acids = len(self.amino_acids)       # 20 classes for targets
        self.vocab_size = self.num_amino_acids + 2         # PAD + UNK + 20 AAs

        # Learned embedding for indexed tokens
        self.aa_embedding = nn.Embedding(self.vocab_size, 128, padding_idx=self.pad_idx)

        # Bidirectional LSTM for sequence processing
        self.lstm = nn.LSTM(
            128, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0,
            bidirectional=True
        )

        # Attention mechanism
        self.attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1)
        )

        # Encode to latent space
        self.fc_latent = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Decoder for reconstruction (for autoencoder pretraining) → 20 AA logits
        self.decoder_lstm = nn.LSTM(
            latent_dim, hidden_dim, num_layers,
            batch_first=True, dropout=dropout if num_layers > 1 else 0
        )
        self.fc_decode = nn.Linear(hidden_dim, self.num_amino_acids)

    def sequences_to_indices(self, sequences, max_len=1000):
        """
        Tokenize and pad sequences to fixed length.
        Args:
            sequences: List of raw strings (AAs). Non-letter chars are stripped.
            max_len: Truncation/padding length.
        Returns:
            idx: LongTensor of shape [B, T] with PAD/UNK/AA indices.
            mask: FloatTensor of shape [B, T], 1.0 for non-PAD positions, else 0.0.
            lengths: LongTensor of shape [B] with unpadded lengths.
        Shapes:
            B = batch size, T = max_len.
        """
        batch_size = len(sequences)
        idx = torch.full((batch_size, max_len), self.pad_idx, dtype=torch.long, device=device)
        lengths = []

        for i, seq in enumerate(sequences):
            s = ""
            if isinstance(seq, str):
                s = re.sub('[^A-Za-z]', '', seq).upper()
            tokens = [self.aa_to_idx.get(ch, self.unk_idx) for ch in s[:max_len]]
            if tokens:
                L = len(tokens)
                idx[i, :L] = torch.tensor(tokens, dtype=torch.long, device=device)
                lengths.append(L)
            else:
                lengths.append(0)

        lengths = torch.tensor(lengths, dtype=torch.long, device=device)
        mask = (idx != self.pad_idx).float()
        return idx, mask, lengths

    def sequence_to_onehot(self, sequences, max_len=500):
        """
        Build fast 20-way one-hot from token indices for standard AAs (ignores PAD/UNK).
        Args:
            sequences: List of raw strings (AAs).
            max_len: Truncation/padding length.

        Returns:
            onehot: FloatTensor [B, T, 20] with 1.0 at AA class, else 0.0.
        """
        idx, _, _ = self.sequences_to_indices(sequences, max_len=max_len)  # [B,T]
        b, t = idx.shape
        out = torch.zeros(b, t, self.num_amino_acids, device=device)       # [B,T,20]
        valid = (idx >= 2)  # only standard AAs
        if valid.any():
            cls = (idx - 2).clamp_min(0)                                   # map 2..21 → 0..19
            out.scatter_(2, cls.unsqueeze(-1), valid.unsqueeze(-1).float())
        return out

    # + full-length helper (no truncation; strips non-letters)
    def cleaned_lengths(self, sequences):
        """
        Return full lengths of sequences after cleaning (no max_len truncation).
        """
        lens = []
        for seq in sequences:
            s = re.sub('[^A-Za-z]', '', seq).upper() if isinstance(seq, str) else ""
            lens.append(len(s))
        return torch.tensor(lens, dtype=torch.long, device=device)

    def encode(self, sequences):
        """
        Encode batch of sequences into latent features.
        Args:
            sequences: List[str] of length B
        Returns:
            latent: FloatTensor [B, latent_dim].
        Notes:
            Uses pack_padded_sequence for efficiency and masked attention to ignore PADs.
        """
        idx, mask, lengths = self.sequences_to_indices(sequences)           # [B,T], [B,T], [B]
        embedded = self.aa_embedding(idx)                                   # [B,T,128]

        # Clamp zero-length samples to length 1 to satisfy pack_padded_sequence
        lengths_clamped = lengths.clamp_min(1)

        # Packed LSTM for efficiency
        packed = nn.utils.rnn.pack_padded_sequence(
            embedded, lengths_clamped.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_out, _ = self.lstm(packed)
        lstm_out, _ = nn.utils.rnn.pad_packed_sequence(
            packed_out, batch_first=True, total_length=idx.size(1)
        )  # [B,T,2H]

        # Masked attention over time (ignore PAD positions)
        attn_scores = self.attention(lstm_out).squeeze(-1)                  # [B,T]
        attn_scores = attn_scores.masked_fill(mask == 0, -1e9)
        attn_weights = F.softmax(attn_scores, dim=1).unsqueeze(-1)          # [B,T,1]
        attended = torch.sum(attn_weights * lstm_out, dim=1)                # [B,2H]

        latent = self.fc_latent(attended)                                   # [B,D]
        return latent

    def decode(self, latent, seq_len=500):
        """
        Decode latent features back to per-position AA logits.
        Args:
            latent: FloatTensor [B, latent_dim].
            seq_len: Target decoded length (T).
        Returns:
            aa_logits: FloatTensor [B, T, 20] (unnormalized).
        """
        batch_size = latent.size(0)
        latent_expanded = latent.unsqueeze(1).repeat(1, seq_len, 1)
        decoded, _ = self.decoder_lstm(latent_expanded)
        aa_logits = self.fc_decode(decoded)                                 # [B,T,20]
        return aa_logits


    def forward(self, sequences, decode=False):
        """
        Forward pass.
        Args:
            sequences: List[str] of length B.
            decode: If True, also return per-position AA logits for autoencoding.
        Returns:
            latent: FloatTensor [B, latent_dim] (always).
            decoded: FloatTensor [B, T, 20] if decode=True.
        """
        latent = self.encode(sequences)
        if decode:
            decoded = self.decode(latent)
            return latent, decoded
        return latent


class ProteinGAN(nn.Module):
    """
    Sequence-aware GAN with Transformer G/D producing per-position AA distributions.
    - G: z -> [B, T, d_model] via linear + learned pos emb + Transformer, head -> 20-way softmax per pos.
    - D: [B, T, 20] -> proj to d_model + pos emb + Transformer, pooled critic score (no sigmoid).
    """
    def __init__(self, latent_dim=64, seq_len=100, hidden_dim=[256, 128], cond_dim=None):
        super().__init__()
        self.latent_dim = latent_dim
        self.seq_len = seq_len
        self.num_amino_acids = 20
        self.d_model = D_MODEL
        self.cond_dim = cond_dim

        # Need DirectSequenceEncoder instance to get 'M' index
        temp_encoder = DirectSequenceEncoder()
        self.m_aa_idx = temp_encoder.amino_acids.find('M') # Get the 0-indexed position in "ACDEFGHIKLMNPQRSTVWY"


        # Generator
        self.z_to_seq = nn.Linear(latent_dim, seq_len * self.d_model)
        self.g_pos = nn.Parameter(torch.zeros(1, seq_len, self.d_model))
        g_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, nhead=NHEAD, dim_feedforward=self.d_model * 4,
            dropout=DROPOUT, batch_first=True
        )
        self.g_tr = nn.TransformerEncoder(g_layer, num_layers=NUM_LAYERS)
        self.g_head = nn.Linear(self.d_model, self.num_amino_acids)
        # Optional conditioning for G
        if self.cond_dim is not None:
            self.g_cond_fc = nn.Linear(self.cond_dim, self.d_model)

        # Discriminator (critic)
        self.d_proj_in = spectral_norm(nn.Linear(self.num_amino_acids, self.d_model))
        self.d_pos = nn.Parameter(torch.zeros(1, seq_len, self.d_model))
        d_layer = nn.TransformerEncoderLayer(
            d_model=self.d_model, nhead=NHEAD, dim_feedforward=self.d_model * 4,
            dropout=DROPOUT, batch_first=True
        )
        self.d_tr = nn.TransformerEncoder(d_layer, num_layers=NUM_LAYERS)
        self.d_head = spectral_norm(nn.Linear(self.d_model, 1))
        # Optional conditioning for D
        if self.cond_dim is not None:
            self.d_cond_fc = nn.Linear(self.cond_dim, self.d_model)

        # init
        nn.init.normal_(self.g_pos, mean=0.0, std=0.02)
        nn.init.normal_(self.d_pos, mean=0.0, std=0.02)
        for m in [self.z_to_seq, self.g_head, self.d_proj_in, self.d_head]:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)
        # init cond layers if present
        for m in getattr(self, "g_cond_fc", []), getattr(self, "d_cond_fc", []):
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight); nn.init.zeros_(m.bias)

    def generate(self, batch_size, cond):
      """
      Sample sequences from the generator: [B, T, 20] probabilities.
      Enforces the first amino acid to be 'M' (index 10).
      """
      z = torch.randn(batch_size, self.latent_dim, device=device)
      x = self.z_to_seq(z).view(batch_size, self.seq_len, self.d_model)
      x = x + self.g_pos

      if cond is not None and hasattr(self, "g_cond_fc"):
          c = self.g_cond_fc(cond)
          c_expanded = c.unsqueeze(1).expand(-1, self.seq_len, -1)
          x = x + c_expanded

      x = self.g_tr(x)  # Don't forget this!
      logits = self.g_head(x)  # [B,T,20]

      # === Enforce first position is 'M' (index 10) ===
      # Set all positions to very negative
      logits[:, 0, :] = -100.0
      # Set M (index 10) to positive
      logits[:, 0, 10] = 10.0  # Hardcode index 10 for M

      probs = F.softmax(logits, dim=-1)

      # Debug: Verify first position
      if batch_size > 0:
          first_idx = probs[0, 0, :].argmax().item()
          if first_idx != 10:
              print(f"WARNING: First position is {self.amino_acids[first_idx]} not M!")

      return probs

    def discriminate(self, sequences, cond=None):
        """
        Critic score for real/fake sequences (no sigmoid).
        Args:
            sequences [B, T, 20] one-hot/probabilities.
            cond: Optional [B, cond_dim] conditioning vector.
        Returns: [B,1] critic scores.
        """
        b = sequences.size(0)
        x = self.d_proj_in(sequences) + self.d_pos           # [B,T,d]
        if cond is not None and hasattr(self, "d_cond_fc"):
            c = self.d_cond_fc(cond).unsqueeze(1)             # [B,1,d]
            x = x + c
        x = self.d_tr(x)                                      # [B,T,d]
        x = x.mean(dim=1)                                     # global average pool
        score = self.d_head(x)                                # [B,1]
        return score


# --- EMA helpers for generator ---
def _gen_param_items(gan: ProteinGAN):
    for name, p in gan.named_parameters():
        if name.startswith("z_to_seq") or name.startswith("g_pos") or name.startswith("g_tr") or name.startswith("g_head") or name.startswith("g_cond_fc"):
            yield name, p

def init_generator_ema(gan: ProteinGAN):
    return {name: p.detach().clone().to(device) for name, p in _gen_param_items(gan)}

def update_generator_ema(ema, gan: ProteinGAN, decay=EMA_DECAY):
    for name, p in _gen_param_items(gan):
        ema[name].mul_(decay).add_(p.detach(), alpha=1.0 - decay)

def load_generator_ema_into_model(gan: ProteinGAN, ema):
    with torch.no_grad():
        for name, p in _gen_param_items(gan):
            p.copy_(ema[name].to(p.device))

class PULearningPPIModel(nn.Module):
    """
    MLP for PPI prediction in a PU-learning setting.
    Args:
        sequence_feature_dim: Dimension of concatenated sequence features (e.g., 4*latent_dim).
        structural_feature_dim: Dimension of structural features (99) in the pretarining datatset.
        hidden_dims: List of hidden layer sizes.
    Input/Output:
        forward(sequence_features [B, S], structural_features [B, D]) -> logits [B, 1]
    """
    def __init__(self, sequence_feature_dim, structural_feature_dim, hidden_dims=[256, 128]):
        super().__init__()
        total_input_dim = sequence_feature_dim + structural_feature_dim
        # Build network layers
        layers = []
        prev_dim = total_input_dim

        for hidden_dim in hidden_dims:
            layers.extend([
                nn.Linear(prev_dim, hidden_dim),
                nn.BatchNorm1d(hidden_dim),
                nn.ReLU(),
                nn.Dropout(0.4)
            ])
            prev_dim = hidden_dim

        # Final layer
        layers.append(nn.Linear(prev_dim, 1))

        self.network = nn.Sequential(*layers)

    def forward(self, sequence_features, structural_features):
        """
        Concatenate inputs and compute interaction logits.
        Args:
            sequence_features: FloatTensor [B, S].
            structural_features: FloatTensor [B, D].
        Returns:
            logits: FloatTensor [B, 1].
        """
        x = torch.cat([sequence_features, structural_features], dim=1)
        return self.network(x)


class PULoss(nn.Module):
    """
    Non-negative PU (nnPU) loss per Kiryo et al., treating unlabeled as a mixture.
    Args:
        prior: Estimated positive prior π_p = P(y=1) in unlabeled data.
        loss_type: 'nnPU' for non-negative correction, otherwise unbiased PU.
    Notes:
        Expects raw logits; applies sigmoid internally.
    """
    def __init__(self, prior=0.05, loss_type='nnPU'):
        super().__init__()
        self.prior = prior
        self.loss_type = loss_type

    def forward(self, outputs, labels):
        """
        Correct nnPU (Kiryo et al.) with BCE-with-logits.
        prior = P(y=1) in unlabeled mixture.
        """
        # flatten
        logits = outputs.view(-1)
        y = labels.view(-1)
        pos_mask = (y == 1)
        unl_mask = (y == 0)

        # risks
        if pos_mask.any():
            pos_logits = logits[pos_mask]
            R_p_pos = F.binary_cross_entropy_with_logits(
                pos_logits, torch.ones_like(pos_logits), reduction='mean'
            )
            R_p_neg = F.binary_cross_entropy_with_logits(
                pos_logits, torch.zeros_like(pos_logits), reduction='mean'
            )
        else:
            R_p_pos = torch.tensor(0.0, device=logits.device)
            R_p_neg = torch.tensor(0.0, device=logits.device)

        if unl_mask.any():
            unl_logits = logits[unl_mask]
            R_u_neg = F.binary_cross_entropy_with_logits(
                unl_logits, torch.zeros_like(unl_logits), reduction='mean'
            )
        else:
            R_u_neg = torch.tensor(0.0, device=logits.device)

        # unbiased negative risk and nnPU clamp
        neg_risk_unbiased = R_u_neg - self.prior * R_p_neg
        neg_risk = torch.clamp(neg_risk_unbiased, min=0.0)

        # total risk
        loss = self.prior * R_p_pos + neg_risk
        return loss


class ProteinDataset(Dataset):
    """
    Dataset wrapper for PPI pairs with sequences and numeric structural covariates.

    Args:
        df: pandas.DataFrame containing columns SequenceA, SequenceB, optional MSAA/MSAB,
            numeric structural columns, and 'label' (1=positive, 0=unlabeled).
        sequence_encoder: Unused here (kept for API symmetry).
        structural_cols: List of column names for structural features.

    Notes:
        - Structural features are taken from the provided `structural_cols`.
    """
    def __init__(self, df, structural_cols, sequence_encoder=None):
        self.df = df.reset_index(drop=True)
        self.sequence_encoder = sequence_encoder
        self.structural_cols = structural_cols

    def __len__(self):
        """Number of samples."""
        return len(self.df)

    def __getitem__(self, idx):
        """
        Get a single item.
        Args:
            idx: Row index.
        Returns:
            dict with:
                - 'sequences': [seqA, seqB, msaA, msaB] as strings.
                - 'structural_features': FloatTensor of structural features.
                - 'label': FloatTensor scalar (0 or 1).
        """
        row = self.df.iloc[idx]

        # Get sequences, ensure strings and not NaN
        seqA = row['SequenceA'] if ('SequenceA' in self.df.columns and isinstance(row['SequenceA'], str)) else ""
        seqB = row['SequenceB'] if ('SequenceB' in self.df.columns and isinstance(row['SequenceB'], str)) else ""
        msaA = row['MSAA'] if ('MSAA' in self.df.columns and isinstance(row['MSAA'], str)) else ""
        msaB = row['MSAB'] if ('MSAB' in self.df.columns and isinstance(row['MSAB'], str)) else ""

        sequences = [seqA, seqB, msaA, msaB]

        # Get structural features
        if self.structural_cols:
            vals = row[self.structural_cols].values.astype(np.float32, copy=False)
            structural_vec = np.nan_to_num(vals, nan=0.0)
        else:
            structural_vec = np.zeros(0, dtype=np.float32)

        structural_tensor = torch.from_numpy(structural_vec)

        # Get label (1 for positive, 0 for unlabeled/negative)
        label = float(row['label']) if 'label' in self.df.columns else 0

        return {
            'sequences': sequences,
            'structural_features': structural_tensor,
            'labels': torch.tensor(label, dtype=torch.float32) # Fix: Use 'label' key (singular)
        }


def collate_fn(batch):
    """
    Collate a list of dataset items into a batch.
    Args:
        batch: List[dict] from ProteinDataset.__getitem__.
    Returns:
        dict with:
            - 'sequences': list of 4 lists (A, B, MSAA, MSAB), each length B.
            - 'structural_features': FloatTensor [B, num_structural_features].
            - 'labels': FloatTensor [B].
    """
    sequences_batch = [[], [], [], []]  # 4 sequence types

    for item in batch:
        for i, seq in enumerate(item['sequences']):
            sequences_batch[i].append(seq)

    return {
        'sequences': sequences_batch,
        'structural_features': torch.stack([item['structural_features'] for item in batch]),
        'labels': torch.stack([item['labels'] for item in batch]) # Fix: Use 'labels' key
    }


def compute_adaptive_gradient_penalty(discriminator_fn, real_samples, fake_samples, device=device):
    """
    Adaptive gradient penalty that adjusts lambda based on gradient magnitude.
    discriminator_fn: callable(tensor[B,T,20]) -> tensor[B,1]
    """
    b = real_samples.size(0)
    eps = torch.rand(b, 1, 1, device=device)
    interpolates = eps * real_samples + (1 - eps) * fake_samples
    interpolates.requires_grad_(True)

    with sdpa_kernel([SDPBackend.MATH]):
        d_interpolates = discriminator_fn(interpolates)
    grads = torch.autograd.grad(
        outputs=d_interpolates,
        inputs=interpolates,
        grad_outputs=torch.ones_like(d_interpolates),
        create_graph=True,
        retain_graph=True,
        only_inputs=True
    )[0]

    grads = grads.view(b, -1)
    grad_norm = grads.norm(2, dim=1)

    # Adaptive lambda based on gradient magnitude
    adaptive_lambda = 10.0 * (1.0 + (grad_norm.mean() - 1.0).abs())

    # Calculate the gradient penalty term
    gradient_penalty = torch.mean((grad_norm - 1)**2)


    return adaptive_lambda * gradient_penalty

def train_gan(gan, dataloader, encoder, num_epochs):
    """
    Improved GAN training with better stability measures.
    Uses PUL context: cond = [encode(A)||encode(B)||encode(MSAA)||encode(MSAB)||struct].
    """
    print("\nTraining GAN with improved stability measures...")

    # Separate optimizers with different learning rates
    gen_params = [p for name, p in gan.named_parameters()
                  if any(name.startswith(prefix) for prefix in ["z_to_seq", "g_pos", "g_tr", "g_head", "g_cond_fc"])]
    disc_params = [p for name, p in gan.named_parameters()
                   if any(name.startswith(prefix) for prefix in ["d_proj_in", "d_pos", "d_tr", "d_head", "d_cond_fc"])]

    g_optimizer = optim.Adam(gen_params, lr=G_LR, betas=G_BETAS)
    d_optimizer = optim.Adam(disc_params, lr=D_LR, betas=D_BETAS)

    # Learning rate schedulers for stability
    g_scheduler = optim.lr_scheduler.CosineAnnealingLR(g_optimizer, T_max=num_epochs * len(dataloader))
    d_scheduler = optim.lr_scheduler.CosineAnnealingLR(d_optimizer, T_max=num_epochs * len(dataloader))

    # Using modern AMP API
    scaler = torch.amp.GradScaler('cuda', enabled=(device.type == "cuda" and AMP_ENABLED))

    g_losses, d_losses = [], []
    temp_encoder = DirectSequenceEncoder().to(device)

    # EMA for generator
    ema = init_generator_ema(gan)

    # Add noise decay schedule
    noise_std = 0.1  # Initial noise standard deviation
    noise_decay = 0.95  # Decay factor per epoch


    for epoch in range(num_epochs):
        g_loss_epoch = 0.0
        d_loss_epoch = 0.0
        # + extra diagnostics
        wdist_epoch = 0.0
        gp_epoch = 0.0
        dreal_epoch = 0.0
        dfake_epoch = 0.0
        num_batches = 0
        # + full-length tracking
        full_len_sum = 0.0
        full_len_count = 0
        # Track samples used
        total_positive_samples = 0
        total_negative_samples = 0
        # Update noise level
        current_noise = noise_std * (noise_decay ** epoch)

        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f'GAN Epoch {epoch+1}/{num_epochs}')):
            all_seqA = batch['sequences'][0]
            labels = batch['labels'].cpu().numpy()

            valid_idx = [i for i, s in enumerate(all_seqA)
                      if isinstance(s, str) and len(s) > 0]
            if len(valid_idx) < 2:
                continue

            # Identify positive and negative samples
            positive_idx = [i for i in valid_idx if labels[i] == 1]
            negative_idx = [i for i in valid_idx if labels[i] == 0]

            # This ensures we don't have a massive imbalance between pos/neg samples
            if len(positive_idx) > 0 and len(negative_idx) > 0:
                # Option: Undersample the majority class to balance
                if len(positive_idx) > len(negative_idx) * 3:
                    # If positives are more than 3x negatives, subsample positives
                    positive_idx = random.sample(positive_idx, len(negative_idx) * 3)
                elif len(negative_idx) > len(positive_idx) * 3:
                    # If negatives are more than 3x positives, subsample negatives
                    negative_idx = random.sample(negative_idx, len(positive_idx) * 3)

            # Using both positive and negative samples => only positive(to be updated later)
            combined_idx = positive_idx + negative_idx

            # Skip if not enough samples
            if len(combined_idx) < 2:
                continue

            # Track statistics
            total_positive_samples += len(positive_idx)
            total_negative_samples += len(negative_idx)

            # Collect corresponding sequences for all 4 types using the combined indices
            real_seqA = [batch['sequences'][0][i] for i in combined_idx]
            real_seqB = [batch['sequences'][1][i] for i in combined_idx]
            real_msaA = [batch['sequences'][2][i] for i in combined_idx]
            real_msaB = [batch['sequences'][3][i] for i in combined_idx]
            struct = batch['structural_features'][combined_idx].to(device)

            # Build conditioning vector with all sequence encodings + structural features
            with torch.no_grad():
                enc_a = encoder(real_seqA)      # [B, LATENT_DIM]
                enc_b = encoder(real_seqB)      # [B, LATENT_DIM]
                enc_msaA = encoder(real_msaA)   # [B, LATENT_DIM]
                enc_msaB = encoder(real_msaB)   # [B, LATENT_DIM]
            cond = torch.cat([enc_a, enc_b, enc_msaA, enc_msaB, struct], dim=1)  # [B, 4*LATENT_DIM + struct_dim]

            # Build real samples from SequenceA as before
            real_onehot = encoder.sequence_to_onehot(real_seqA, max_len=gan.seq_len)
            cur_bs = real_onehot.size(0)

            # Add noise to real samples (label smoothing)
            if current_noise > 0:
                noise = torch.randn_like(real_onehot) * current_noise
                real_onehot_noisy = real_onehot + noise
                real_onehot_noisy = real_onehot_noisy.clamp(0, 1)
            else:
                real_onehot_noisy = real_onehot

            # + compute full lengths (no truncation)
            full_lens = encoder.cleaned_lengths(real_seqA)
            full_len_sum += full_lens.sum().item()
            full_len_count += full_lens.numel()

            # Train Discriminator
            for _ in range(N_CRITIC):
                d_optimizer.zero_grad(set_to_none=True)

                with torch.amp.autocast('cuda', enabled=(device.type == "cuda" and AMP_ENABLED)):
                    with sdpa_kernel([SDPBackend.MATH]):
                        fake = gan.generate(cur_bs, cond=cond).detach()
                        fake_noisy = fake + torch.randn_like(fake) * current_noise * 0.5
                        d_real = gan.discriminate(real_onehot_noisy, cond=cond).mean()
                        d_fake = gan.discriminate(fake_noisy, cond=cond).mean()
                        # Wasserstein loss (raw, without GP)
                        d_loss = d_fake - d_real

                # Gradient penalty (computed in fp32; internal call also uses math attention)
                gp = compute_adaptive_gradient_penalty(lambda x: gan.discriminate(x, cond=cond),
                                                       real_onehot, fake, device=device)

                # Total discriminator loss
                d_total = d_loss + gp

                scaler.scale(d_total).backward()

                # Gradient clipping for discriminator
                torch.nn.utils.clip_grad_norm_(disc_params, max_norm=1.0)

                scaler.step(d_optimizer)
                scaler.update()

            # Train Generator
            g_optimizer.zero_grad(set_to_none=True)
            with torch.amp.autocast('cuda', enabled=(device.type == "cuda" and AMP_ENABLED)):
                with sdpa_kernel([SDPBackend.MATH]):
                    fake = gan.generate(cur_bs, cond=cond)
                    g_loss = -gan.discriminate(fake, cond=cond).mean()

            scaler.scale(g_loss).backward()

            # Gradient clipping for generator
            torch.nn.utils.clip_grad_norm_(gen_params, max_norm=1.0)

            scaler.step(g_optimizer)
            scaler.update()

            # Update EMA
            update_generator_ema(ema, gan, decay=EMA_DECAY)

            # Update schedulers
            g_scheduler.step()
            d_scheduler.step()

            # After updates, accumulate diagnostics from the last critic step
            g_loss_epoch += g_loss.item()
            d_loss_epoch += d_total.item()
            # + diagnostics (detach to CPU-safe scalars)
            wdist_epoch += (d_real - d_fake).detach().item()
            gp_epoch += gp.detach().item()
            dreal_epoch += d_real.detach().item()
            dfake_epoch += d_fake.detach().item()
            num_batches += 1

            # Early stopping check every 10 batches
            if batch_idx % 10 == 0 and batch_idx > 0:
                if abs(d_loss.item()) < 0.01:  # Discriminator too strong
                    print(f"Warning: Discriminator loss too low ({d_loss.item():.4f}), potential mode collapse")
                if g_loss.item() > 10:  # Generator failing
                    print(f"Warning: Generator loss too high ({g_loss.item():.4f})")

        if num_batches > 0:
            avg_g_loss = g_loss_epoch / num_batches
            avg_d_loss = d_loss_epoch / num_batches
            avg_wdist = wdist_epoch / num_batches
            avg_gp = gp_epoch / num_batches
            avg_dreal = dreal_epoch / num_batches
            avg_dfake = dfake_epoch / num_batches
            g_losses.append(avg_g_loss)
            d_losses.append(avg_d_loss)
            avg_full_len = (full_len_sum / max(1, full_len_count))
            print(f'Epoch {epoch+1}: G_Loss: {avg_g_loss:.4f}, D_Loss(total): {avg_d_loss:.4f}, Noise: {current_noise:.4f} | AvgSeqLen(full)={avg_full_len:.1f}')
            print(f'  Positive samples: {total_positive_samples} | Negative samples: {total_negative_samples} | Batches: {num_batches}')
            print(f'  Critic: W-dist≈{avg_wdist:.4f} | D_real={avg_dreal:.4f} | D_fake={avg_dfake:.4f} | GP={avg_gp:.4f}')



    # Attach EMA to model
    gan._ema = ema
    return g_losses, d_losses

def train_model(model, encoder, dataloader, val_loader, num_epochs):
    """
    Train the PPI model with PU loss, jointly optimizing the encoder.

    Args:
        model: PULearningPPIModel.
        encoder: DirectSequenceEncoder.
        dataloader: Training DataLoader.
        val_loader: Validation DataLoader.
        num_epochs: Number of epochs.

    Returns:
        (train_losses, val_losses): Lists of average losses per epoch.
    """
    print("\nTraining PPI model with PU learning...")

    optimizer = optim.Adam(
        list(model.parameters()) + list(encoder.parameters()),
        lr=0.00003,
        weight_decay=1e-5
    )

    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)

    # PU learning loss: prior is expected fraction of positives in unlabeled mixture
    pu_loss = PULoss(prior=0.05)

    train_losses = []
    val_losses = []

    for epoch in range(num_epochs):
        # Training
        model.train()
        encoder.train()
        total_loss = 0

        for batch in tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}'):
            optimizer.zero_grad()

            # Encode sequences
            encoded_seqs = []
            for seq_type in batch['sequences']:
                encoded = encoder(seq_type)
                encoded_seqs.append(encoded)

            # Concatenate all sequence encodings
            seq_features = torch.cat(encoded_seqs, dim=1)

            # Get predictions
            outputs = model(seq_features, batch['structural_features'].to(device))

            loss = pu_loss(outputs.squeeze(-1), batch['labels'].to(device, non_blocking=True))

            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)
            optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss / len(dataloader)
        train_losses.append(avg_train_loss)

        # Validation
        model.eval()
        encoder.eval()
        val_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                encoded_seqs = []
                for seq_type in batch['sequences']:
                    encoded = encoder(seq_type)
                    encoded_seqs.append(encoded)

                seq_features = torch.cat(encoded_seqs, dim=1)
                outputs = model(seq_features, batch['structural_features'].to(device, non_blocking=True))
                loss = pu_loss(outputs.squeeze(-1), batch['labels'].to(device, non_blocking=True))
                val_loss += loss.item()
        avg_val_loss = val_loss / len(val_loader)
        val_losses.append(avg_val_loss)
        scheduler.step(avg_val_loss)
        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')

    return train_losses, val_losses


def evaluate_model(model, encoder, dataloader, dataset_name="Test"):
    """
    Evaluate model performance with ranking and threshold metrics using sklearn.
    Args:
        model: PULearningPPIModel.
        encoder: DirectSequenceEncoder.
        dataloader: DataLoader for evaluation split.
        dataset_name: Label for logs/plots.

    Returns:
        dict with AUC, AUPR, precision, recall, f1, accuracy, confusion_matrix,
        predictions, labels; or None if only one class present.
    """
    print(f"\n{'='*60}")
    print(f"Evaluating on {dataset_name} Dataset")
    print(f"{'='*60}")

    model.eval()
    encoder.eval()

    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f'Evaluating {dataset_name}'):
            encoded_seqs = []
            for seq_type in batch['sequences']:
                encoded = encoder(seq_type)
                encoded_seqs.append(encoded)

            seq_features = torch.cat(encoded_seqs, dim=1)
            outputs = model(seq_features, batch['structural_features'].to(device, non_blocking=True))
            probs = torch.sigmoid(outputs.squeeze(-1))

            all_labels.extend(batch['labels'].cpu().numpy())
            all_predictions.extend(probs.cpu().numpy())

    all_labels = np.array(all_labels)
    all_predictions = np.array(all_predictions)

    if len(np.unique(all_labels)) < 2:
        print("Only one class present in labels; skipping ROC-based metrics.")
        return None

    # AUC-ROC
    auc = roc_auc_score(all_labels, all_predictions)
    print(f"AUC-ROC: {auc:.4f}")

    # PR-AUC
    aupr = average_precision_score(all_labels, all_predictions)
    print(f"AUPR: {aupr:.4f}")

    # Calculate metrics at a fixed threshold (0.92)
    fixed_threshold = 0.6
    binary_pred_fixed = (all_predictions > fixed_threshold).astype(int)

    precision_fixed = precision_score(all_labels, binary_pred_fixed)
    recall_fixed = recall_score(all_labels, binary_pred_fixed)
    f1_fixed = f1_score(all_labels, binary_pred_fixed)
    accuracy_fixed = accuracy_score(all_labels, binary_pred_fixed)
    cm_fixed = confusion_matrix(all_labels, binary_pred_fixed)


    print(f"\nMetrics at fixed threshold ({fixed_threshold:.2f}):")
    print(f"Precision: {precision_fixed:.4f}")
    print(f"Recall:    {recall_fixed:.4f}")
    print(f"F1 Score:  {f1_fixed:.4f}")
    print(f"Accuracy:  {accuracy_fixed:.4f}")
    print(f"Confusion Matrix:\n{cm_fixed}")


    # Plot distribution
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.hist(all_predictions[all_labels==1], bins=30, alpha=0.5, label='Positive', color=COLOR_PALETTE[0])
    plt.hist(all_predictions[all_labels==0], bins=30, alpha=0.5, label='Unlabeled', color=COLOR_PALETTE[1])
    plt.xlabel('Prediction Score')
    plt.ylabel('Count')
    plt.legend()
    plt.title(f'{dataset_name} Score Distribution')

    plt.subplot(1, 2, 2)
    fpr, tpr, _ = roc_curve(all_labels, all_predictions)
    plt.plot(fpr, tpr, label=f'AUC = {auc:.3f}', color=COLOR_PALETTE[2])
    plt.plot([0, 1], [0, 1], 'k--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'{dataset_name} ROC Curve')
    plt.legend()

    plt.tight_layout()
    plt.show()

    # PR curve
    precision_arr, recall_arr, _ = precision_recall_curve(all_labels, all_predictions)
    plt.figure(figsize=(5,4))
    plt.plot(recall_arr, precision_arr, color=COLOR_PALETTE[0], label=f'AUPR = {aupr:.3f}')
    plt.xlabel('Recall'); plt.ylabel('Precision'); plt.title(f'{dataset_name} Precision-Recall')
    plt.legend(); plt.tight_layout(); plt.show()

    return {
        'auc': auc,
        'aupr': aupr,
        'precision': precision_fixed,
        'recall': recall_fixed,
        'f1': f1_fixed,
        'accuracy': accuracy_fixed,
        'confusion_matrix': cm_fixed,
        'predictions': all_predictions,
        'labels': all_labels,
    }



import os
from typing import Dict, Any, Optional

import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE  # Import SMOTE


def load_and_prepare_data(
    data_path: str = "/content/drive/MyDrive/pretraining.csv",
    batch_size: int = 128,
    unlabeled_weight: float = 5.0,
    num_workers: int = 0,
    persistent_workers: bool = False,
    device: Optional[torch.device] = None,
    apply_smote: bool = True,  # New parameter to control SMOTE application
    random_state: int = 42,    # For reproducibility
) -> Dict[str, Any]:
    """
    Load data, split into train/val/test, scale features (fit on train only),
    apply SMOTE to balance training data, and build DataLoaders.
    Also returns a positives-only train DataLoader for GANs.

    Returns:
      {
        'train_loader': DataLoader,
        'val_loader': DataLoader,
        'test_loader': DataLoader,
        'pos_train_loader': DataLoader,  # positives-only (label==1)
        'train_df': DataFrame,
        'val_df': DataFrame,
        'test_df': DataFrame,
        'structural_feature_names': List[str],
        'scaler': StandardScaler
      }
    """
    print("="*60)
    print("Loading and Preparing Data")
    print("="*60)

    if device is None:
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    pin_mem = (device.type == "cuda")

    # 1) Load
    print("\nLoading dataset...")
    df = pd.read_csv(data_path)
    print(f"Dataset shape: {df.shape}")

    # 2) Filter sequences
    df_filtered = df.dropna(subset=["SequenceA", "SequenceB"], how="all").copy()
    print(f"Filtered dataset shape: {df_filtered.shape}")

    # 3) Clean 'label' column for stratification

    df_filtered.dropna() # Ensure no NaNs remain after conversion

    # 3.1) Label distribution (after cleaning)
    print("\nLabel distribution (1=Positive, 0=Unlabeled/Negative) after cleaning:")
    print(df_filtered["label"].value_counts())


    # 4) Structural features: keep stable order and intersect with existing columns
    candidate_features = ['Volume', 'CZ', 'CA', 'O', 'OD1', 'OG', 'N', 'NZ', 'DU', 'CZ40', 'CZ40-50',
                         'CZ50-60', 'CZ60-70', 'CZ70-80', 'CZ80-90', 'CZ90-100', 'CZ100-110', 'CZ110-120',
                         'CZ120', 'CA40', 'CA40-50', 'CA50-60', 'CA60-70', 'CA70-80', 'CA80-90',
                         'CA90-100', 'CA100-110', 'CA110-120', 'CA120', 'O40', 'O40-50', 'O50-60',
                         'O60-70', 'O70-80', 'O80-90', 'O90-100', 'O100-110', 'O110-120', 'O120',
                         'OD140', 'OD140-50', 'OD150-60', 'OD160-70', 'OD170-80', 'OD180-90',
                         'OD190-100', 'OD1100-110', 'OD1110-120', 'OD1120', 'OG40', 'OG40-50',
                         'OG50-60', 'OG60-70', 'OG70-80', 'OG80-90', 'OG90-100', 'OG100-110',
                         'OG110-120', 'OG120', 'N40', 'N40-50', 'N50-60', 'N60-70', 'N70-80',
                         'N80-90', 'N90-100', 'N100-110', 'N110-120', 'N120', 'NZ40', 'NZ40-50',
                         'NZ50-60', 'NZ60-70', 'NZ70-80', 'NZ80-90', 'NZ90-100', 'NZ100-110',
                         'NZ110-120', 'NZ120', 'DU40', 'DU40-50', 'DU50-60', 'DU60-70', 'DU70-80',
                         'DU80-90', 'DU90-100', 'DU100-110', 'DU110-120', 'DU120',
                         'T40', 'T40-50', 'T50-60', 'T60-70', 'T70-80', 'T80-90', 'T90-100',
                         'T100-110', 'T110-120', 'PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2', 'Rgyr',
                         'Asphericity', 'SpherocityIndex', 'Eccentricity', 'InertialShapeFactor']

    structural_feature_names = [c for c in candidate_features if c in df_filtered.columns]
    if not structural_feature_names:
        raise ValueError("No structural feature columns found in the input CSV.")

    # 5) Split (stratified)
    if len(df_filtered["label"].unique()) < 2:
         print("Warning: Only one class present in 'label' after cleaning. Cannot perform stratified split.")
         # Fallback to non-stratified split or handle as needed
         train_df, temp_df = train_test_split(
            df_filtered, test_size=0.3, random_state=random_state
         )
    else:
        train_df, temp_df = train_test_split(
            df_filtered, test_size=0.3, random_state=random_state, stratify=df_filtered["label"]
        )
        if len(temp_df["label"].unique()) < 2:
            print("Warning: Only one class present in temp_df after first split. Cannot perform stratified validation split.")
            val_df, test_df = train_test_split(
                temp_df, test_size=0.5, random_state=random_state
            )
        else:
            val_df, test_df = train_test_split(
                temp_df, test_size=0.5, random_state=random_state, stratify=temp_df["label"]
            )


    print("\nDataset splits:")
    print(f"Train: {train_df.shape[0]} samples")
    print(f"Validation: {val_df.shape[0]} samples")
    print(f"Test: {test_df.shape[0]} samples")

    # 6) Fit scaler on train only (no leakage), then transform val/test
    scaler = StandardScaler()
    train_df.loc[:, structural_feature_names] = scaler.fit_transform(train_df[structural_feature_names])
    val_df.loc[:, structural_feature_names] = scaler.transform(val_df[structural_feature_names])
    test_df.loc[:, structural_feature_names] = scaler.transform(test_df[structural_feature_names])

    # 7) Apply SMOTE to training data only
    if apply_smote:
        print("\nApplying SMOTE to balance training data...")
        # Store the original index and non-feature columns
        X_train = train_df[structural_feature_names].values
        y_train = train_df["label"].values

        # Record original class distribution
        unique, counts = np.unique(y_train, return_counts=True)
        print(f"Original training distribution: {dict(zip(unique, counts))}")

        # Apply SMOTE
        if len(unique) > 1 and np.min(counts) >= 2:
            smote = SMOTE(random_state=random_state)
            X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

            # Record new class distribution
            unique_res, counts_res = np.unique(y_train_resampled, return_counts=True)
            print(f"After SMOTE: {dict(zip(unique_res, counts_res))}")


            non_feature_cols = [col for col in train_df.columns if col not in structural_feature_names]

            # Create a new DataFrame with the synthetic data
            train_df_resampled = pd.DataFrame(X_train_resampled, columns=structural_feature_names)
            train_df_resampled["label"] = y_train_resampled

            original_count = len(train_df)
            synthetic_count = len(train_df_resampled) - original_count

            if synthetic_count > 0:
                print(f"Created {synthetic_count} synthetic samples with SMOTE")

                # Copy original samples' non-feature data
                for col in non_feature_cols:
                    if col != "label":  # Label was already handled
                        # For original samples, keep their values
                        if len(train_df) <= len(train_df_resampled):
                            train_df_resampled.loc[:len(train_df)-1, col] = train_df[col].values

                        # For synthetic samples, we can either:
                        # 1. Use values from their nearest neighbors (complex)
                        # 2. Use default values or NaNs (simpler)
                        # Here we'll use a simple approach - sample with replacement from the original samples of the same class
                        for class_val in np.unique(y_train_resampled):
                            # Find original samples of this class
                            orig_class_samples_df = train_df[train_df["label"] == class_val]

                            # Find indices of synthetic samples of this class in the resampled dataframe
                            synth_indices = train_df_resampled[
                                (train_df_resampled["label"] == class_val) &
                                (train_df_resampled.index >= original_count) # Check index if original_count is used as offset
                            ].index

                            if len(synth_indices) > 0 and not orig_class_samples_df.empty:
                                # Sample with replacement from the original samples of this class
                                if col in orig_class_samples_df.columns:
                                     sampled_values = np.random.choice(
                                        orig_class_samples_df[col].dropna().values,
                                        size=len(synth_indices),
                                        replace=True
                                    )
                                     train_df_resampled.loc[synth_indices, col] = sampled_values
                                else:
                                     train_df_resampled.loc[synth_indices, col] = pd.NA

            train_df = train_df_resampled
        else:
            print("SMOTE skipped: Not enough samples in minority class for resampling.")


    # 8) Datasets
    train_dataset = ProteinDataset(train_df, structural_feature_names)
    val_dataset = ProteinDataset(val_df, structural_feature_names)
    test_dataset = ProteinDataset(test_df, structural_feature_names)

    # 9)
    if apply_smote and len(df_filtered["label"].unique()) > 1 and np.min(counts) >= 2:
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            collate_fn=collate_fn,
            pin_memory=pin_mem,
            num_workers=num_workers,
            persistent_workers=persistent_workers,
            shuffle=True,
            drop_last=False,
        )
    else:
        labels_np = train_df["label"].to_numpy()
        # Only apply weighting if there are both 0 and 1 labels in the training set
        if len(np.unique(labels_np)) > 1:
             weights = np.where(labels_np == 1, 1.0, float(unlabeled_weight))
             sampler = WeightedRandomSampler(weights=weights, num_samples=len(weights), replacement=True)
             train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                collate_fn=collate_fn,
                pin_memory=pin_mem,
                num_workers=num_workers,
                persistent_workers=persistent_workers,
                sampler=sampler,
                drop_last=False,
             )
        else:
             # If only one class exists after filtering/SMOTE issues, use basic shuffling
             print("Warning: Only one class in training set after processing. Using standard shuffle instead of WeightedRandomSampler.")
             train_loader = DataLoader(
                train_dataset,
                batch_size=batch_size,
                collate_fn=collate_fn,
                pin_memory=pin_mem,
                num_workers=num_workers,
                persistent_workers=persistent_workers,
                shuffle=True,
                drop_last=False,
            )


    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        pin_memory=pin_mem,
        num_workers=num_workers,
        persistent_workers=persistent_workers,
        shuffle=False,
        drop_last=False,
    )
    test_loader = DataLoader(
        test_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        pin_memory=pin_mem,
        num_workers=num_workers,
        persistent_workers=persistent_workers,
        shuffle=False,
        drop_last=False,
    )

    # 10) Positives-only train DataLoader (for GAN)
    pos_train_df = train_df[train_df["label"] == 1].copy()
    pos_train_dataset = ProteinDataset(pos_train_df, structural_feature_names)
    pos_train_loader = DataLoader(
        pos_train_dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        pin_memory=pin_mem,
        num_workers=num_workers,
        persistent_workers=persistent_workers,
        shuffle=True,   # shuffle for GAN
        drop_last=True  # helps GAN stability
    )

    print(f"Dataloader batches - Train: {len(train_loader)}, Val: {len(val_loader)}, Test: {len(test_loader)}")
    print(f"Positives-only train batches (GAN): {len(pos_train_loader)}")

    return {
        "train_loader": train_loader,
        "val_loader": val_loader,
        "test_loader": test_loader,
        "pos_train_loader": pos_train_loader,
        "train_df": train_df,
        "val_df": val_df,
        "test_df": test_df,
        "structural_feature_names": structural_feature_names,
        "scaler": scaler,
    }

def run_autoencoder_pu_pipeline():
    """Run the autoencoder pretraining and PU learning pipeline."""
    print("="*60)
    print("AUTOENCODER + PU LEARNING PIPELINE")
    print("="*60)

    # Load data
    data_dict = load_and_prepare_data()
    train_loader = data_dict['train_loader']
    val_loader = data_dict['val_loader']
    test_loader = data_dict['test_loader']
    structural_feature_names = data_dict['structural_feature_names']

    # Initialize encoder
    print("\n" + "="*60)
    print("Initializing Encoder")
    print("="*60)

    LATENT_DIM = 64
    encoder = DirectSequenceEncoder(
        latent_dim=LATENT_DIM,
        hidden_dim=256,
        num_layers=2,
        dropout=0.2
    ).to(device)

    print(f"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}")

    # Initialize PPI model
    sequence_feature_dim = LATENT_DIM * 4  # 4 sequence types
    structural_feature_dim = len(structural_feature_names)

    ppi_model = PULearningPPIModel(
        sequence_feature_dim=sequence_feature_dim,
        structural_feature_dim=structural_feature_dim,
        hidden_dims=[256]
    ).to(device)

    print(f"PPI Model parameters: {sum(p.numel() for p in ppi_model.parameters()):,}")

    # Phase 1: Pre-train encoder with autoencoding
    print("\n" + "="*60)
    print("Phase 1: Pre-training Encoder with Autoencoding")
    print("="*60)

    ae_optimizer = optim.Adam(encoder.parameters(), lr=0.001)
    ae_train_losses = []  # Store autoencoder training losses

    for epoch in range(40):
        encoder.train()
        total_loss = 0
        for batch in tqdm(train_loader, desc=f'Autoencoder Epoch {epoch+1}/40'):
            ae_optimizer.zero_grad()

            # Use first sequence type for autoencoding
            sequences = batch['sequences'][0]

            # Encode and decode
            latent, decoded = encoder(sequences, decode=True)

            # Reconstruction loss
            target_onehot = encoder.sequence_to_onehot(sequences)
            loss = F.mse_loss(decoded, target_onehot)

            loss.backward()
            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)
            ae_optimizer.step()

            total_loss += loss.item()

        avg_train_loss = total_loss/len(train_loader)
        ae_train_losses.append(avg_train_loss)  # Store average loss per epoch
        print(f'Epoch {epoch+1}: Reconstruction Loss: {avg_train_loss:.4f}')

    print("Encoder pre-training complete!")

    # Phase 1 plots: Autoencoder training curves
    plt.figure(figsize=(10, 5))
    ax = sns.lineplot(x=range(1, len(ae_train_losses)+1), y=ae_train_losses, label='AE Train Loss', color=COLOR_PALETTE[0])
    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.set_title('Autoencoder Pre-training Loss')
    ax.legend()
    style_axes(ax)
    plt.tight_layout()
    save_fig('ae_training_curves.png')
    plt.show()


    # Phase 2: Train PPI model with PU learning
    print("\n" + "="*60)
    print("Phase 2: Training PPI Model with PU Learning")
    print("="*60)

    train_losses, val_losses = train_model(
        ppi_model, encoder, train_loader, val_loader, num_epochs=50
    )

    # Phase 2 plots: PPI training curves
    plt.figure(figsize=(10, 5))
    ax = sns.lineplot(x=range(1, len(train_losses)+1), y=train_losses, label='Train Loss', color=COLOR_PALETTE[0])
    ax = sns.lineplot(x=range(1, len(val_losses)+1), y=val_losses, label='Validation Loss', color=COLOR_PALETTE[1])
    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.set_title('PPI Model Training with PU Learning')
    ax.legend()
    style_axes(ax)
    plt.tight_layout()
    save_fig('pu_training_curves.png')
    plt.show()

    # Evaluate models
    print("\n" + "="*60)
    print("Evaluation")
    print("="*60)

    # Evaluate on training data
    print("\nEvaluating on TRAINING data...")
    train_results = evaluate_model(ppi_model, encoder, train_loader, "Training")

    # Evaluate on test data
    print("\nEvaluating on TEST data...")
    test_results = evaluate_model(ppi_model, encoder, test_loader, "Test")

    # Evaluate on Alzheimer's proteins
    alzheimer_results = evaluate_alzheimer_proteins(ppi_model, encoder, structural_feature_names)

    # Save models
    print("\n" + "="*60)
    print("Saving Models")
    print("="*60)

    torch.save({
        'encoder': encoder.state_dict(),
        'ppi_model': ppi_model.state_dict(),
        'train_metrics': train_results,
        'test_metrics': test_results,
        'alzheimer_results': alzheimer_results,
        'ae_train_losses': ae_train_losses, # Save autoencoder losses
        'train_losses': train_losses,
        'val_losses': val_losses
    }, os.path.join(get_output_dir(), 'autoencoder_pu_complete.pth'))

    print("Autoencoder + PU models saved successfully!")

    return {
        'encoder': encoder,
        'ppi_model': ppi_model,
        'results': {
            'train': train_results,
            'test': test_results,
            'alzheimer': alzheimer_results,
            'ae_train_losses': ae_train_losses, # Return autoencoder losses
            'train_losses': train_losses,
            'val_losses': val_losses
        }
    }

def run_gan_pipeline(pretrained_encoder=None):
    """Run the GAN training pipeline."""
    print("="*60)
    print("GAN TRAINING PIPELINE")
    print("="*60)

    # Load data
    data_dict = load_and_prepare_data()
    train_loader = data_dict['pos_train_loader']
    structural_feature_names = data_dict['structural_feature_names']

    # Initialize or load encoder
    LATENT_DIM = 64
    ppi_model_path = os.path.join(get_output_dir(), 'autoencoder_pu_complete.pth')
    ppi_checkpoint = torch.load(ppi_model_path, map_location=device, weights_only=False)
    LATENT_DIM = 64

    ppi_state = ppi_checkpoint['ppi_model']
    first_layer_weight = ppi_state['network.0.weight']
    total_input_dim = first_layer_weight.shape[1]

    encoder = DirectSequenceEncoder(
    latent_dim=LATENT_DIM,
    hidden_dim=256,
    num_layers=2,
    dropout=0.2
    ).to(device)

    print(f"Encoder parameters: {sum(p.numel() for p in encoder.parameters()):,}")

    # Initialize GAN
    print("\n" + "="*60)
    print("Initializing GAN")
    print("="*60)

    structural_feature_dim = len(structural_feature_names)
    gan = ProteinGAN(
        latent_dim=64,
        seq_len=100,
        hidden_dim=[256, 128],
        cond_dim=LATENT_DIM * 4 + structural_feature_dim
    ).to(device)

    # Calculate GAN parameter counts
    gen_params_count = sum(
        p.numel() for n, p in gan.named_parameters()
        if n.startswith(("z_to_seq", "g_pos", "g_tr", "g_head", "g_cond_fc"))
    )
    disc_params_count = sum(
        p.numel() for n, p in gan.named_parameters()
        if n.startswith(("d_proj_in", "d_pos", "d_tr", "d_head", "d_cond_fc"))
    )

    print(f"GAN Generator parameters: {gen_params_count:,}")
    print(f"GAN Discriminator parameters: {disc_params_count:,}")

    # Train GAN
    print("\n" + "="*60)
    print("Training GAN for Sequence Generation")
    print("="*60)


    g_losses, d_losses = train_gan(gan, train_loader, encoder, num_epochs=80)

    # GAN training curves
    fig, ax = plt.subplots(figsize=(10, 5))
    epochs = range(1, len(g_losses) + 1)
    ax.plot(epochs, g_losses, label='Generator Loss', color=GEN_COLOR, linestyle=GEN_LS, linewidth=2.0)
    ax.plot(epochs, d_losses, label='Discriminator Loss', color=DISC_COLOR, linestyle=DISC_LS, linewidth=2.0)
    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss'); ax.set_title('GAN Training Progress')
    ax.legend()
    style_axes(ax)
    plt.tight_layout()
    save_fig('gan_training_curves.png')
    plt.show()

    # Generate novel sequences
    print("\n" + "="*60)
    print("Generating Novel Protein Sequences with GAN")
    print("="*60)

    # Generate sample sequences using the GAN
    with torch.no_grad():
        gan.eval()
        sample_probs = gan.generate(10, cond=None)  # Generate 10 sample sequences
        amino_acids = "ACDEFGHIKLMNPQRSTVWY"
        novel_sequences = []
        for i in range(sample_probs.size(0)):

            indices = torch.argmax(sample_probs[i], dim=-1)
            seq = "".join(amino_acids[idx] for idx in indices if idx < len(amino_acids))
            novel_sequences.append(seq[:200])  # Truncate for display

    print(f"Generated {len(novel_sequences)} novel sequences (first 200 chars shown):")
    for i, seq in enumerate(novel_sequences[:3]):  # Show first 3
        print(f"  Seq {i+1}: {seq}...")

    # Save GAN models
    print("\n" + "="*60)
    print("Saving GAN Models")
    print("="*60)

    torch.save({
        'encoder': encoder.state_dict(),
        'gan': gan.state_dict(),
        'gan_ema': getattr(gan, "_ema", None),
        'g_losses': g_losses,
        'd_losses': d_losses,
        'novel_sequences': novel_sequences
    }, os.path.join(get_output_dir(), 'gan_complete.pth'))

    print("GAN models saved successfully!")

    return {
        'encoder': encoder,
        'gan': gan,
        'results': {
            'g_losses': g_losses,
            'd_losses': d_losses,
            'novel_sequences': novel_sequences
        }
    }

def load_alzheimer_dataset():
    """Load and examine the Alzheimer interactome dataset"""
    data_path = '/content/drive/MyDrive/Alzheimer.csv'

    try:
        df = pd.read_csv(data_path)
        display(df.head())


        # Clean the dataset
        df_clean = df.dropna(subset=['SequenceA', 'SequenceB']).copy()

        return df_clean

    except FileNotFoundError:
        print(f"Error: Could not find {data_path}")
        print("Please ensure Alzheimer.csv is in  Google Drive root folder")
        return None

def evaluate_alzheimer_proteins(model, encoder, structural_feature_names, sample_size=None, batch_size=32, threshold=0.5):
    """
    Evaluate protein-protein interactions from the Alzheimer dataset with improved efficiency and functionality.

    Args:
        model: PULearningPPIModel - Trained PPI prediction model
        encoder: DirectSequenceEncoder - Trained sequence encoder
        structural_feature_names: List[str] - Names of structural feature columns
        sample_size: Optional[int] - Number of pairs to evaluate (None = all pairs)
        batch_size: int - Batch size for processing (default: 32)
        threshold: float - Probability threshold for binary classification (default: 0.5)

    Returns:
        Dict containing:
            - results: List[dict] with pair predictions
            - metrics: Dict with evaluation metrics
            - visualizations: Generated plots
    """
    print("\n" + "="*60)
    print("EVALUATING ALZHEIMER PROTEIN-PROTEIN INTERACTIONS")
    print("="*60)

    # Load dataset with error handling
    try:
        df = load_alzheimer_dataset()
        if df is None or df.empty:
            print("Error: No Alzheimer dataset loaded")
            return None
    except Exception as e:
        print(f"Error loading Alzheimer dataset: {e}")
        return None

    print(f"Loaded dataset with {len(df)} protein pairs")

    # Sample if requested
    if sample_size and sample_size < len(df):
        df = df.sample(n=sample_size, random_state=42).reset_index(drop=True)
        print(f"Sampling {sample_size} pairs for evaluation")


    model.eval()
    encoder.eval()

    results = []
    processing_errors = 0

    # Process in batches for memory efficiency
    print(f"Processing {len(df)} pairs in batches of {batch_size}...")

    with torch.no_grad():
        for batch_start in tqdm(range(0, len(df), batch_size), desc="Processing batches"):
            batch_end = min(batch_start + batch_size, len(df))
            batch_df = df.iloc[batch_start:batch_end]

            # Prepare batch data
            batch_sequences = [[], [], [], []]  # 4 sequence types
            batch_pairs = []
            batch_valid_indices = []

            for idx, (_, row) in enumerate(batch_df.iterrows()):
                # Validate sequences
                if pd.isna(row['SequenceA']) or pd.isna(row['SequenceB']):
                    processing_errors += 1
                    continue

                seqA = str(row['SequenceA']).strip()
                seqB = str(row['SequenceB']).strip()

                if len(seqA) == 0 or len(seqB) == 0:
                    processing_errors += 1
                    continue

                # Add to batch
                batch_sequences[0].append(seqA)    # SequenceA
                batch_sequences[1].append(seqB)    # SequenceB
                batch_sequences[2].append("")      # Empty MSAA
                batch_sequences[3].append("")      # Empty MSAB

                batch_pairs.append({
                    'UniProt_A': row['UniProt_A'],
                    'UniProt_B': row['UniProt_B'],
                    'seq_len_A': len(seqA),
                    'seq_len_B': len(seqB)
                })
                batch_valid_indices.append(idx)

            if not batch_sequences[0]:  # No valid sequences in batch
                continue

            try:
                # Encode sequences in batch
                encoded_seqs = []
                for seq_type in batch_sequences:
                    encoded = encoder(seq_type)
                    encoded_seqs.append(encoded)

                seq_features = torch.cat(encoded_seqs, dim=1)

                # Create structural features (zeros for dummy features)
                batch_size_actual = len(batch_sequences[0])
                struct_features = torch.zeros(batch_size_actual, len(structural_feature_names), device=device)

                # Predict interactions
                outputs = model(seq_features, struct_features)
                probabilities = torch.sigmoid(outputs.squeeze(-1))

                # Store results
                for i, (pair_info, prob) in enumerate(zip(batch_pairs, probabilities)):
                    pair_name = f"{pair_info['UniProt_A']}-{pair_info['UniProt_B']}"

                    results.append({
                        'pair': pair_name,
                        'UniProt_A': pair_info['UniProt_A'],
                        'UniProt_B': pair_info['UniProt_B'],
                        'probability': prob.item(),
                        'predicted': prob.item() > threshold,
                        'seq_len_A': pair_info['seq_len_A'],
                        'seq_len_B': pair_info['seq_len_B'],
                        'confidence': 'high' if abs(prob.item() - 0.5) > 0.3 else 'low'
                    })

            except Exception as e:
                print(f"Error processing batch {batch_start}-{batch_end}: {e}")
                processing_errors += len(batch_sequences[0])
                continue

    print(f"Processing complete. Errors encountered: {processing_errors}")

    if not results:
        print("No valid results generated")
        return None

    # Sort by probability (highest first)
    results.sort(key=lambda x: x['probability'], reverse=True)

    # Generate analysis and metrics
    analysis = _analyze_alzheimer_results(results, threshold)

    # Create visualizations
    visualizations = _create_alzheimer_visualizations(results, threshold)

    return {
        'results': results,
        'metrics': analysis,
        'visualizations': visualizations,
        'processing_errors': processing_errors,
        'total_processed': len(results)
    }


def _analyze_alzheimer_results(results, threshold=0.92):
    """Analyze results and compute detailed metrics."""
    positive_predictions = [r for r in results if r['predicted']]
    negative_predictions = [r for r in results if not r['predicted']]

    # Basic statistics
    total_pairs = len(results)
    pos_count = len(positive_predictions)
    neg_count = len(negative_predictions)

    # Probability statistics
    all_probs = [r['probability'] for r in results]
    prob_stats = {
        'mean': np.mean(all_probs),
        'std': np.std(all_probs),
        'median': np.median(all_probs),
        'min': np.min(all_probs),
        'max': np.max(all_probs)
    }

    # Confidence distribution
    conf_counts = {'high': 0, 'low': 0}
    for r in results:
        conf_counts[r['confidence']] += 1

    # Sequence length statistics
    seq_len_stats = {
        'mean_len_A': np.mean([r['seq_len_A'] for r in results]),
        'mean_len_B': np.mean([r['seq_len_B'] for r in results]),
        'total_residues': sum(r['seq_len_A'] + r['seq_len_B'] for r in results)
    }

    # Print detailed analysis
    print("\n" + "="*60)
    print("DETAILED ANALYSIS RESULTS")
    print("="*60)

    print(f"Total pairs evaluated: {total_pairs}")
    print(f"Predicted interactions (YES): {pos_count} ({pos_count/total_pairs*100:.1f}%)")
    print(f"Predicted non-interactions (NO): {neg_count} ({neg_count/total_pairs*100:.1f}%)")
    print(f"High confidence predictions: {conf_counts['high']} ({conf_counts['high']/total_pairs*100:.1f}%)")
    print(f"Low confidence predictions: {conf_counts['low']} ({conf_counts['low']/total_pairs*100:.1f}%)")


    print(f"\nProbability Statistics:")
    print(f"  Mean: {prob_stats['mean']:.4f}")
    print(f"  Std:  {prob_stats['std']:.4f}")
    print(f"  Range: [{prob_stats['min']:.4f}, {prob_stats['max']:.4f}]")

    # Top predictions
    print("\n" + "="*60)
    print("TOP 10 PREDICTED INTERACTIONS")
    print("-" * 60)
    for i, res in enumerate(positive_predictions[:10], 1):
        print(f"{i:2}. {res['pair']:30} | Prob: {res['probability']:.4f} | "
              f"Conf: {res['confidence']:4} | Lengths: {res['seq_len_A']:3d}, {res['seq_len_B']:3d}")

    if not positive_predictions:
        print("No positive interactions predicted")

    # Bottom predictions
    print("\n" + "="*60)
    print("TOP 10 CONFIDENT NON-INTERACTIONS")
    print("-" * 60)
    bottom_predictions = sorted(negative_predictions, key=lambda x: x['probability'])[:10]
    for i, res in enumerate(bottom_predictions, 1):
        print(f"{i:2}. {res['pair']:30} | Prob: {res['probability']:.4f} | "
              f"Conf: {res['confidence']:4} | Lengths: {res['seq_len_A']:3d}, {res['seq_len_B']:3d}")

    return {
        'total_pairs': total_pairs,
        'positive_predictions': pos_count,
        'negative_predictions': neg_count,
        'probability_stats': prob_stats,
        'confidence_stats': conf_counts,
        'sequence_stats': seq_len_stats,
        'threshold_used': threshold
    }


def _create_alzheimer_visualizations(results, threshold=0.92):
    """Create comprehensive visualizations for Alzheimer results."""
    import matplotlib.pyplot as plt
    import seaborn as sns

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Probability distribution
    probs = [r['probability'] for r in results]
    axes[0, 0].hist(probs, bins=30, alpha=0.7, color=COLOR_PALETTE[0], edgecolor='black')
    axes[0, 0].axvline(np.mean(probs), color='orange', linestyle='--', label=f'Mean: {np.mean(probs):.3f}')
    axes[0, 0].set_xlabel('Interaction Probability')
    axes[0, 0].set_ylabel('Count')
    axes[0, 0].set_title('Distribution of Interaction Probabilities')
    style_axes(axes[0, 0]) # Apply styling to individual axis
    axes[0, 0].legend()

    # 2. Sequence length distribution
    len_A = [r['seq_len_A'] for r in results]
    len_B = [r['seq_len_B'] for r in results]

    from matplotlib.colors import LinearSegmentedColormap
    custom_colors = COLOR_PALETTE[:3]
    custom_cmap = LinearSegmentedColormap.from_list("custom", custom_colors)

    scatter = axes[0, 1].scatter(len_A, len_B, alpha=0.6, c=probs, cmap=custom_cmap, s=20)
    axes[0, 1].set_xlabel('Sequence A Length')
    axes[0, 1].set_ylabel('Sequence B Length')
    axes[0, 1].set_title('Sequence Lengths vs Interaction Probability')
    style_axes(axes[0, 1]) # Apply styling to individual axis
    cbar = plt.colorbar(scatter, ax=axes[0, 1]) # Use the scatter object for colorbar
    cbar.set_label('Interaction Probability')


    # 3. Confidence distribution
    conf_counts = {'high': 0, 'low': 0}
    for r in results:
        conf_counts[r['confidence']] += 1

    axes[1, 0].bar(conf_counts.keys(), conf_counts.values(),
                   color=[COLOR_PALETTE[0], COLOR_PALETTE[1]], alpha=0.7)
    axes[1, 0].set_ylabel('Count')
    axes[1, 0].set_title('Prediction Confidence Distribution')
    style_axes(axes[1, 0]) # Apply styling to individual axis


    # 4. Top interactions network-style plot
    top_results = sorted(results, key=lambda x: x['probability'], reverse=True)[:20]
    probs_top = [r['probability'] for r in top_results]
    x_pos = range(len(top_results))

    bars = axes[1, 1].bar(x_pos, probs_top, color=COLOR_PALETTE[2], alpha=0.7)
    axes[1, 1].axhline(threshold, color='red', linestyle='--', alpha=0.7)
    axes[1, 1].set_xlabel('Protein Pair Rank')
    axes[1, 1].set_ylabel('Interaction Probability')
    axes[1, 1].set_title('Top 20 Protein Pair Predictions')
    style_axes(axes[1, 1]) # Apply styling to individual axis
    plt.tight_layout()

    # Save visualization
    viz_path = save_fig('alzheimer_evaluation_analysis.png')
    plt.show()

    print(f"Visualization saved to: {viz_path}")

    return {
        'plot_path': viz_path,
        'figure': fig
    }

def load_trained_models_for_alzheimer():
    """Load  pre-trained models for Alzheimer analysis"""

    LATENT_DIM = 64

    # Load checkpoints
    gan_model_path = os.path.join(get_output_dir(), 'gan_complete.pth')
    ppi_model_path = os.path.join(get_output_dir(), 'autoencoder_pu_complete.pth')

    if not os.path.exists(gan_model_path):
        print(f"Error: GAN model file not found at {gan_model_path}")
        print("Please run GAN training script first to create the models")
        return None, None, None, None
    if not os.path.exists(ppi_model_path):
        print(f"Error: PPI model file not found at {ppi_model_path}")
        print("Please run Autoencoder + PU pipeline script first to create the models")
        return None, None, None, None

    # Load checkpoints
    try:
        gan_checkpoint = torch.load(gan_model_path, map_location=device, weights_only=False)
        ppi_checkpoint = torch.load(ppi_model_path, map_location=device, weights_only=False)
    except Exception as e:
        print(f"Error loading with weights_only=False: {e}")
        try:
            import numpy
            torch.serialization.add_safe_globals([numpy.core.multiarray.scalar])
            gan_checkpoint = torch.load(gan_model_path, map_location=device, weights_only=True)
            ppi_checkpoint = torch.load(ppi_model_path, map_location=device, weights_only=True)
            print("Loaded checkpoints with safe globals")
        except Exception as e2:
            print(f"Failed to load checkpoints: {e2}")
            return None, None, None, None

    # Inspect the PPI model structure to get actual dimensions
    ppi_state = ppi_checkpoint['ppi_model']
    first_layer_weight = ppi_state['network.0.weight']
    total_input_dim = first_layer_weight.shape[1]


    sequence_feature_dim = LATENT_DIM * 4  # 4 sequence types = 256
    structural_feature_dim = total_input_dim - sequence_feature_dim

    print(f"Detected dimensions:")
    print(f"  Total input: {total_input_dim}")
    print(f"  Sequence features: {sequence_feature_dim}")
    print(f"  Structural features: {structural_feature_dim}")


    # Initialize models with correct dimensions
    encoder = DirectSequenceEncoder(
        latent_dim=LATENT_DIM,
        hidden_dim=256,
        num_layers=2,
        dropout=0.2
    ).to(device)

    ppi_model = PULearningPPIModel(
        sequence_feature_dim=sequence_feature_dim,
        structural_feature_dim=structural_feature_dim,
        hidden_dims=[256]
    ).to(device)

    gan = ProteinGAN(
        latent_dim=LATENT_DIM,
        seq_len=100,
        hidden_dim=256,
        cond_dim=LATENT_DIM * 4 + structural_feature_dim
    ).to(device)

    # Load the model states
    try:
        encoder.load_state_dict(gan_checkpoint['encoder']) # Encoder is saved in GAN checkpoint
        ppi_model.load_state_dict(ppi_checkpoint['ppi_model']) # PPI model in its own checkpoint
        gan.load_state_dict(gan_checkpoint['gan']) # GAN in its own checkpoint


        # Load EMA weights if available
        if 'gan_ema' in gan_checkpoint and gan_checkpoint['gan_ema'] is not None:
            gan._ema = gan_checkpoint['gan_ema']

        print("Models loaded successfully!")
        return encoder, ppi_model, gan, structural_feature_dim

    except Exception as e:
        print(f"Error loading model states: {e}")
        return None, None, None, None


def generate_interacting_proteins_for_alzheimer_target(
    gan, encoder, ppi_model, target_name, target_sequence,
    structural_feature_dim, num_candidates=1000, top_k=50, min_prob=0.92
):
    """
    Generate novel proteins that are predicted to interact with a specific Alzheimer protein.

    This version uses CONDITIONAL generation, where the target protein's own
    encoded features are used as a condition for the GAN. This guides the
    generator to produce sequences that are more likely to be relevant to the target.

    Args:
        target_name: Name of target protein (e.g., 'APP', 'TAU')
        target_sequence: Amino acid sequence of target protein
        num_candidates: Number of candidates to generate and test
        top_k: Number of top candidates to return
        min_prob: Minimum interaction probability threshold
    """
    print(f"\n{'='*80}")
    print(f"GENERATING INTERACTING PROTEINS FOR {target_name}")
    print(f"{'='*80}")


    gan.eval()
    encoder.eval()
    ppi_model.eval()

    # Load EMA weights if available
    if hasattr(gan, "_ema"):
        load_generator_ema_into_model(gan, gan._ema)

    amino_acids = "ACDEFGHIKLMNPQRSTVWY"
    candidates = []

    # Create the conditioning vector for the target protein ONCE
    with torch.no_grad():
        target_sequences_for_cond = [
            [target_sequence], [target_sequence], [target_sequence], [target_sequence]
        ]
        encoded_seqs_cond = [encoder(seq_type) for seq_type in target_sequences_for_cond]

        # Dummy structural features for the conditioning vector
        struct_features_cond = torch.zeros(1, structural_feature_dim, device=device)

        # The final conditioning vector
        conditioning_vector = torch.cat(encoded_seqs_cond + [struct_features_cond], dim=1)

    def is_repetitive(sequence):
        """Check if a sequence is repetitive"""
        if len(sequence) < 6:
            return True

        # Check for short repeats (4-mers)
        for j in range(len(sequence) - 3):
            if len(set(sequence[j:j+3])) <= 1:  # All same character in 3-mer
                return True

        # Check for longer repeats (6-mers with low diversity)
        #for j in range(len(sequence) - 4):
         #   if len(set(sequence[j:j+5])) <= 2:  # Low diversity in 6-mer
          #      return True

        # Check for runs of 5+ identical characters
        #if len(sequence) >= 5:
         #   for j in range(len(sequence) - 3):
          #      if sequence[j] == sequence[j+1] == sequence[j+2] == sequence[j+3] == sequence[j+4]:
           #         return True

        return False

    def generate_single_sequence():
        """Generate a single non-repetitive sequence"""
        max_attempts = 10  # Maximum attempts to generate a non-repetitive sequence

        for attempt in range(max_attempts):
            # Generate single sequence
            batch_cond_vector = conditioning_vector  # [1, cond_dim]
            generated_probs = gan.generate(1, cond=batch_cond_vector)  # [1, T, 20]

            pos_probs = generated_probs[0]  # [T, 20]

            temperature = 1.2 + (attempt * 0.1)  # Increase temperature with each attempt
            token_indices = _sample_tokens_from_probs(pos_probs, temperature=temperature, top_k=10)
            generated_seq = "".join(amino_acids[idx] for idx in token_indices
                                  if idx < len(amino_acids))

            # Check if sequence is valid (not repetitive)
            if not is_repetitive(generated_seq):
                return generated_seq

        # If all attempts failed, return None
        return None

    with torch.no_grad():
        # Statistics tracking
        total_generated = 0
        total_filtered_short = 0
        total_filtered_repetitive = 0
        total_tested = 0

        pbar = tqdm(total=num_candidates, desc=f"Generating for {target_name}")

        while total_tested < num_candidates:
            # Generate a single sequence
            generated_seq = generate_single_sequence()
            total_generated += 1

            if generated_seq is None:
                total_filtered_repetitive += 1
                continue

            # Test interaction with target protein
            sequences = [
                [target_sequence],    # SequenceA (target)
                [generated_seq],      # SequenceB (generated candidate)
                [""],                 # MSAA (empty)
                [""]                  # MSAB (empty)
            ]

            # Encode all sequences
            try:
                encoded_seqs = []
                for seq_type in sequences:
                    encoded = encoder(seq_type)
                    encoded_seqs.append(encoded)

                seq_features = torch.cat(encoded_seqs, dim=1)

                # Create dummy structural features (zeros, like in training)
                struct_features = torch.zeros(1, structural_feature_dim, device=device)

                # Predict interaction probability
                output = ppi_model(seq_features, struct_features)
                prob = torch.sigmoid(output).item()

                total_tested += 1

                if prob >= min_prob:
                    candidates.append({
                        'target_protein': target_name,
                        'generated_sequence': generated_seq,
                        'interaction_probability': prob,
                        'sequence_length': len(generated_seq),
                        'sequence_id': f'{target_name}_CAND_{len(candidates)+1}'
                    })

                # Update progress bar
                pbar.update(1)

            except Exception as e:
                print(f"Error processing sequence: {e}")
                continue

        pbar.close()

    # Sort by probability (highest first)
    candidates.sort(key=lambda x: x['interaction_probability'], reverse=True)

    # Return top candidates
    top_candidates = candidates[:top_k]

    print(f"\nResults for {target_name}:")
    print(f"  Total generation attempts: {total_generated}")
    print(f"  Filtered (repetitive): {total_filtered_repetitive}")
    print(f"  Successfully tested: {total_tested}")
    print(f"  Candidates above threshold: {len(candidates)}")
    print(f"  Returning top: {len(top_candidates)}")

    if total_filtered_repetitive > 0:
        print(f"  Repetition filter efficiency: {(total_filtered_repetitive/total_generated)*100:.1f}% filtered")

    if top_candidates:
        print(f"\nTop 10 candidates for {target_name}:")
        print("-" * 90)
        for i, candidate in enumerate(top_candidates[:10], 1):
            prob = candidate['interaction_probability']
            seq_len = candidate['sequence_length']
            seq_preview = candidate['generated_sequence'][:50] + "..." if len(candidate['generated_sequence']) > 50 else candidate['generated_sequence']
            print(f"  {i:2d}. Prob: {prob:.4f} | Len: {seq_len:3d} | {seq_preview}")

    return top_candidates

# --- top-k temperature sampling helper (updated to add more randomness) ---
def _sample_tokens_from_probs(probs, temperature=1.0, top_k=10):
    """
    probs: [T, 20] probabilities; returns List[int] sampled per position.
    Updated to add more randomness for diverse sequence generation.
    """
    if temperature <= 0:
        # argmax
        return torch.argmax(probs, dim=-1).tolist()

    logits = torch.log(probs.clamp(min=1e-9)) / temperature  # [T,20]

    if top_k and top_k < logits.size(-1):
        topk_vals, topk_idx = torch.topk(logits, top_k, dim=-1)
        mask = torch.full_like(logits, fill_value=-float('inf'))
        logits = mask.scatter(-1, topk_idx, topk_vals)


    noise = torch.randn_like(logits) * 0.05
    logits = logits + noise

    # Sample from the modified logits
    return torch.distributions.Categorical(logits=logits).sample().tolist()


def check_similarity_with_alzheimer_data(generated_sequences, alzheimer_sequences, threshold=0.3):
    """Check if generated sequences are similar to known Alzheimer proteins"""

    similarities = []

    for i, gen_seq in enumerate(generated_sequences):
        best_similarity = 0.0
        best_match = None

        for alz_seq in alzheimer_sequences:
            # Calculate sequence similarity ratio
            similarity = SequenceMatcher(None, gen_seq, alz_seq).ratio()

            if similarity > best_similarity:
                best_similarity = similarity
                best_match = alz_seq

        if best_similarity >= threshold:
            similarities.append({
                'generated_sequence': gen_seq,
                'alzheimer_sequence': best_match,
                'similarity_score': best_similarity,
                'generated_id': f'GEN_{i+1}'
            })

    print(f"Found {len(similarities)} sequences with similarity >= {threshold}")

    if similarities:
        print("\nTop similarity matches:")
        print("-" * 80)
        for sim in similarities[:5]:  # Show top 5
            gen_preview = sim['generated_sequence'][:40] + "..." if len(sim['generated_sequence']) > 40 else sim['generated_sequence']
            alz_preview = sim['alzheimer_sequence'][:40] + "..." if len(sim['alzheimer_sequence']) > 40 else sim['alzheimer_sequence']
            print(f"  {sim['generated_id']}: Similarity = {sim['similarity_score']:.3f}")
            print(f"    Generated: {gen_preview}")
            print(f"    Alzheimer: {alz_preview}\n")

    return similarities

def get_interaction_partners(target_protein):
    """
    Get all interaction partner sequences for a target protein

    Args:
        target_protein: The protein identifier to look for

    Returns:
        Set of partner sequences
    """
    alzheimer_df = load_alzheimer_dataset()

    if alzheimer_df is None:
        return set()

    partner_sequences = set()

    # Check SequenceA column - if target is here, add SequenceB partners
    mask_A = alzheimer_df['UniProt_A'].str.contains(target_protein, na=False)
    if mask_A.any():
        partners_from_B = alzheimer_df.loc[mask_A, 'SequenceB'].dropna().tolist()
        partner_sequences.update(partners_from_B)

    # Check SequenceB column - if target is here, add SequenceA partners
    mask_B = alzheimer_df['UniProt_B'].str.contains(target_protein, na=False)
    if mask_B.any():
        partners_from_A = alzheimer_df.loc[mask_B, 'SequenceA'].dropna().tolist()
        partner_sequences.update(partners_from_A)
    print(f"Found {len(partner_sequences)} interaction partners for {target_protein}")
    return partner_sequences

def analyze_alzheimer_protein(target_protein, target_sequence=None,
                            num_candidates=1000, top_k=50, min_prob=0.92):
    """
    Main function to analyze a single Alzheimer protein

    Args:
        target_protein: Name of target protein ('APP', 'TAU', 'PSEN1', 'PSEN2')
        target_sequence: Optional custom sequence (uses default if None)
        num_candidates: Number of candidates to generate
        top_k: Number of top results to return
        min_prob: Minimum interaction probability threshold
    """

    # Map protein names to UniProt IDs
    alzheimer_proteins_uniprot = {
        'APP': 'P05067',
        'TAU': 'P10636',
        'PSEN1': 'P49768',
        'MAPT': 'P10636-8'
    }

    # Default Alzheimer protein sequences (actual amino acid sequences)
    alzheimer_proteins_sequences = {
        'APP': 'MLPGLALLLLAAWTARALEVPTDGNAGLLAEPQIAMFCGRLNMHMNVQNGKWDSDPSGTKTCIDTKEGILQYCQEVYPELQITNVVEANQPVTIQNWCKRGRKQCKTHPHFVIPYRCLVGEFVSDALLVPDKCKFLHQERMDVCETHLHWHTVAKETCSEKSTNLHDYGMLLPCGIDKFRGVEFVCCPLAEESDNVDSADAEEDDSDVWWGGADTDYADGSEDKVVEVAEEEEVAEVEEEEADDDEDDEDGDEVEEEAEEPYEEATERTTSIATTTTTTTESVEEVVREVCSEQAETGPCRAMISRWYFDVTEGKCAPFFYGGCGGNRNNFDTEEYCMAVCGSAMSQSLLKTTQEPLARDPVKLPTTAASTPDAVDKYLETPGDENEHAHFQKAKERLEAKHRERMSQVMREWEEAERQAKNLPKADKKAVIQHFQEKVESLEQEAANERQQLVETHMARVEAMLNDRRRLALENYITALQAVPPRPRHVFNMLKKYVRAEQKDRQHTLKHFEHVRMVDPKKAAQIRSQVMTHLRVIYERMNQSLSLLYNVPAVAEEIQDEVDELLKKEQNYSDDVLANMISEPRISYGNDALMPSLTETKTTVELLPVNGEFSLDDLQPWHSFGADSVPANTENEVEPVDARPAADRGLTTRPGSGLTNIKTEEISEVKMDAEFRHDSGYEVHHQKLVFFAEDVGSNKGAIIGLMVGGVVIATVIVITLVMLKKKQYTSIHHGVVEVDAAVTPEERHLSKMQQNGYENPTYKFFEQMQN',
        'TAU': 'MAEPRQEFEVMEDHAGTYGLGDRKDQGGYTMHQDQEGDTDAGLKESPLQTPTEDGSEEPGSETSDAKSTPTAEDVTAPLVDEGAPGKQAAAQPHTEIPEGTTAEEAGIGDTPSLEDEAAGHVTQARMVSKSKDGTGSDDKKAKGADGKTKIATPRGAAPPGQKGQANATRIPAKTPPAPKTPPSSGEPPKSGDRSGYSSPGSPGTPGSRSRTPSLPTPPTREPKKVAVVRTPPKSPSSAKSRLQTAPVPMPDLKNVKSKIGSTENLKHQPGGGKVQIINKKLDLSNVQSKCGSKDNIKHVPGGGSVQIVYKPVDLSKVTSKCGSLGNIHHKPGGGQVEVKSEKLDFKDRVQSKIGSLDNITHVPGGGNKKIETHKLTFRENAKAKTDHGAEIVYKSPVVSGDTSPRHLSNVSSTGSIDMVDSPQLATLADEVSASLAKQGL',
        'PSEN1': 'MTELPAPLSYFQNAQMSEDNHLSNTVRSQNDNRERQEHNDRRSLGHPEPLSNGRPQGNSRQVVEQDEEEDEELTLKYGAKHVIMLFVPVTLCMVVVVATIKSVSFYTRKDGQLIYTPFTEDTETVGQRALHSILNAAIMISVIVVMTILLVVLYKYRCYKVIHAWLIISSLLLLFFFSFIYLGEVFKTYNVAVDYITVALLIWNFGVVGMISIHWKGPLRLQQAYLIMISALMALVFIKYLPEWTAWLILAVISVYDLVAVLCPKGPLRMLVETAQERNETLFPALIYSSTMVWLVNMAEGDPEAQRRVSKNSKYNAESTERESQDTVAENDDGGFSEEWEAQRDSHLGPHRSTPESRAAVQELSSSILAGEDPEERGVKLGLGDFIFYSVLVGKASATASGDWNTTIACFVAILIGLCLTLLLLAIFKKALPALPISITFGLVFYFATDYLVQPFMDQLAFHQFYI',
        'MAPT': 'MAEPRQEFEVMEDHAGTYGLGDRKDQGGYTMHQDQEGDTDAGLKESPLQTPTEDGSEEPGSETSDAKSTPTAEDVTAPLVDEGAPGKQAAAQPHTEIPEGTTAEEAGIGDTPSLEDEAAGHVTQARMVSKSKDGTGSDDKKAKGADGKTKIATPRGAAPPGQKGQANATRIPAKTPPAPKTPPSSGEPPKSGDRSGYSSPGSPGTPGSRSRTPSLPTPPTREPKKVAVVRTPPKSPSSAKSRLQTAPVPMPDLKNVKSKIGSTENLKHQPGGGKVQIINKKLDLSNVQSKCGSKDNIKHVPGGGSVQIVYKPVDLSKVTSKCGSLGNIHHKPGGGQVEVKSEKLDFKDRVQSKIGSLDNITHVPGGGNKKIETHKLTFRENAKAKTDHGAEIVYKSPVVSGDTSPRHLSNVSSTGSIDMVDSPQLATLADEVSASLAKQGL'
    }

    # Load models and data
    encoder, ppi_model, gan, structural_feature_dim = load_trained_models_for_alzheimer()
    if encoder is None:
        return None

    # Get the actual sequence for the target protein
    if target_sequence is None:
        if target_protein.upper() in alzheimer_proteins_sequences:
            target_sequence = alzheimer_proteins_sequences[target_protein.upper()]
        else:
            print(f"Error: No default sequence for {target_protein}")
            print("Available targets:", list(alzheimer_proteins_sequences.keys()))
            return None

    # Get known interaction partners for similarity checking
    alzheimer_partner_sequences = []
    if target_protein.upper() in alzheimer_proteins_uniprot:
        uniprot_id = alzheimer_proteins_uniprot[target_protein.upper()]
        alzheimer_partner_sequences = list(get_interaction_partners(uniprot_id))

    # Generate interacting proteins
    candidates = generate_interacting_proteins_for_alzheimer_target(
        gan, encoder, ppi_model, target_protein, target_sequence,
        structural_feature_dim, num_candidates, top_k, min_prob
    )

    if not candidates:
        print(f"No candidates found for {target_protein} with probability >= {min_prob}")
        return {'target': target_protein, 'candidates': [], 'similarities': []}

    # Check similarities with known Alzheimer interaction partners
    generated_sequences = [c['generated_sequence'] for c in candidates]

    comparison_sequences = alzheimer_partner_sequences if alzheimer_partner_sequences else [target_sequence]

    similarities = check_similarity_with_alzheimer_data(generated_sequences, comparison_sequences, threshold=0.3)

    # Create visualization
    create_alzheimer_analysis_plot(target_protein, candidates, similarities)

    # Summary
    print(f"\n{'='*60}")
    print(f"SUMMARY FOR {target_protein}")
    print(f"{'='*60}")
    print(f"Total candidates found: {len(candidates)}")
    if candidates:
        print(f"Average probability: {np.mean([c['interaction_probability'] for c in candidates]):.3f}")
        print(f"Average sequence length: {np.mean([c['sequence_length'] for c in candidates]):.1f}")
    print(f"Similarity matches: {len(similarities)}")

    return {
        'target': target_protein,
        'target_sequence': target_sequence,
        'candidates': candidates,
        'similarities': similarities,
        'models': {'encoder': encoder, 'ppi_model': ppi_model, 'gan': gan}
    }

def create_alzheimer_analysis_plot(target_protein, candidates, similarities):
    """Create comprehensive visualization for Alzheimer protein analysis"""
    import matplotlib.pyplot as plt
    import seaborn as sns

    fig, axes = plt.subplots(2, 2, figsize=(15, 10))

    # 1. Probability distribution
    probs = [c['interaction_probability'] for c in candidates]
    axes[0, 0].hist(probs, bins=15, color=COLOR_PALETTE[0], alpha=0.7, edgecolor='black')
    axes[0, 0].set_title(f'Interaction Probabilities with {target_protein}')
    axes[0, 0].set_xlabel('Probability')
    axes[0, 0].set_ylabel('Count')
    axes[0, 0].axvline(np.mean(probs), color='red', linestyle='--',
                      label=f'Mean: {np.mean(probs):.3f}')
    axes[0, 0].legend()

    # 2. Sequence lengths
    lengths = [c['sequence_length'] for c in candidates]
    axes[0, 1].hist(lengths, bins=15, color=COLOR_PALETTE[1], alpha=0.7, edgecolor='black')
    axes[0, 1].set_title(f'Generated Sequence Lengths for {target_protein}')
    axes[0, 1].set_xlabel('Sequence Length')
    axes[0, 1].set_ylabel('Count')
    axes[0, 1].axvline(np.mean(lengths), color='red', linestyle='--',
                      label=f'Mean: {np.mean(lengths):.1f}')
    axes[0, 1].legend()

    # 3. Top candidates
    top_10 = candidates[:10]
    x_pos = range(len(top_10))
    top_probs = [c['interaction_probability'] for c in top_10]

    bars = axes[1, 0].bar(x_pos, top_probs, color=COLOR_PALETTE[2], alpha=0.7)
    axes[1, 0].set_title(f'Top 10 Candidates for {target_protein}')
    axes[1, 0].set_xlabel('Candidate Rank')
    axes[1, 0].set_ylabel('Interaction Probability')
    axes[1, 0].set_xticks(x_pos)
    axes[1, 0].set_xticklabels([f'{i+1}' for i in x_pos])

    # Add probability labels on bars
    for bar, prob in zip(bars, top_probs):
        height = bar.get_height()
        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.005,
                       f'{prob:.3f}', ha='center', va='bottom', fontsize=8)

    # 4. Similarity analysis
    if similarities:
        sim_scores = [s['similarity_score'] for s in similarities]
        axes[1, 1].hist(sim_scores, bins=10, color=COLOR_PALETTE[3], alpha=0.7, edgecolor='black')
        axes[1, 1].set_title(f'Similarity with Alzheimer Dataset\n({target_protein})')
        axes[1, 1].set_xlabel('Similarity Score')
        axes[1, 1].set_ylabel('Count')
        axes[1, 1].axvline(np.mean(sim_scores), color='red', linestyle='--',
                          label=f'Mean: {np.mean(sim_scores):.3f}')
        axes[1, 1].legend()
    else:
        axes[1, 1].text(0.5, 0.5, 'No significant\nsimilarity matches\nfound',
                       ha='center', va='center', transform=axes[1, 1].transAxes,
                       fontsize=12, bbox=dict(boxstyle="round,pad=0.3", facecolor="lightgray"))
        axes[1, 1].set_title(f'Similarity with Alzheimer Dataset\n({target_protein})')

    plt.suptitle(f'Generated Protein Analysis for {target_protein}', fontsize=16, fontweight='bold')
    plt.tight_layout()

    # Save the plot
    filename = f'{target_protein.lower()}_alzheimer_analysis.png'
    filepath = os.path.join(get_output_dir(), filename)
    plt.savefig(filepath, dpi=150, bbox_inches='tight')
    plt.show()

    print(f"Analysis plot saved as {filename}")

# Convenience functions for each Alzheimer protein
def analyze_app(num_candidates=1000, top_k=50, min_prob=0.92):
    """Analyze APP interactions"""
    return analyze_alzheimer_protein('APP', num_candidates=num_candidates,
                                   top_k=top_k, min_prob=min_prob)

def analyze_tau(num_candidates=1000, top_k=50, min_prob=0.92):
    """Analyze TAU interactions"""
    return analyze_alzheimer_protein('TAU', num_candidates=num_candidates,
                                   top_k=top_k, min_prob=min_prob)

def analyze_psen1(num_candidates=1000, top_k=50, min_prob=0.92):
    """Analyze PSEN1 interactions"""
    return analyze_alzheimer_protein('PSEN1', num_candidates=num_candidates,
                                   top_k=top_k, min_prob=min_prob)

def analyze_mapt(num_candidates=1000, top_k=50, min_prob=0.92):
    """Analyze PSEN2 interactions"""
    return analyze_alzheimer_protein('MAPT', num_candidates=num_candidates,
                                   top_k=top_k, min_prob=min_prob)

def analyze_all_alzheimer_proteins(num_candidates=500, top_k=30, min_prob=0.92):
    """Analyze all Alzheimer proteins and create comparative report"""
    print("="*80)
    print("COMPREHENSIVE ALZHEIMER PROTEIN ANALYSIS")
    print("="*80)

    proteins = ['APP', 'TAU', 'PSEN1', 'MAPT']
    results = {}

    for protein in proteins:
        print(f"\n{'-'*50}")
        print(f"Analyzing {protein}")
        print(f"{'-'*50}")
        results[protein] = analyze_alzheimer_protein(protein, num_candidates=num_candidates,
                                                   top_k=top_k, min_prob=min_prob)

    # Create comparative summary
    print(f"\n{'='*80}")
    print("COMPARATIVE SUMMARY")
    print(f"{'Protein':<8} {'Candidates':<10} {'Avg Prob':<10} {'Avg Length':<11} {'Similarities':<12}")
    print("-" * 60)

    for protein, result in results.items():
        if result and result['candidates']:
            candidates = result['candidates']
            avg_prob = np.mean([c['interaction_probability'] for c in candidates])
            avg_len = np.mean([c['sequence_length'] for c in candidates])
            similarities = len(result['similarities'])
            print(f"{protein:<8} {len(candidates):<10} {avg_prob:<10.3f} {avg_len:<11.1f} {similarities:<12}")
        else:
            print(f"{protein:<8} {'0':<10} {'N/A':<10} {'N/A':<11} {'0':<12}")

    return results

run_autoencoder_pu_pipeline()

run_gan_pipeline()

analyze_all_alzheimer_proteins(num_candidates=200, top_k=100, min_prob=0.6)